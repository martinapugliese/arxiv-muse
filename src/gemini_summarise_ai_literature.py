# -*- coding: utf-8 -*-
"""Gemini_summarise_AI_literature.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vSsKggNon9HwiY4qx-45H3JySsz7oonF

    Script from the notebook, to be used to pick prod code from.
"""

# Commented out IPython magic to ensure Python compatibility.
# this is to get time execution on each cell
!pip install ipython-autotime

# %load_ext autotime

from google.colab import userdata, drive

from pydantic import BaseModel, Field
from inspect import cleandoc

from google import genai
from google.genai import Client, types

import requests
import urllib.request as urllib_req
from bs4 import BeautifulSoup

from datetime import date, datetime

import re

import os
import shutil
import json

import numpy as np

"""## Configs"""

client = genai.Client(api_key=userdata.get('GEMINI_API_KEY'))

# configure which Gemini to run
model_id = "gemini-2.0-flash-lite-preview-02-05"
model_id = "gemini-2.0-flash"

"""## Scrape ArXiv page for latest day's AI papers

Get the paper links and IDs. Use the most recent days of publications available.
"""

webpage = "https://arxiv.org/list/cs.AI/recent?skip=0&show=2000"  # this is the URL for all, so no need to paginate

r = requests.get(webpage)
r.status_code  # you want a 200 here

# initialise the parser
soup = BeautifulSoup(r.content, "html.parser")

# pick the phrasing of the most recent day
latest_day_str = soup.find_all("h3")[0].text

# match what's this latest day
day = latest_day_str.split('(')[0]

# and the total number of entries for that day
match = re.search(r'of \d+ entries', latest_day_str)
if match:
    n_entries = int(match.group().split(' ')[1])
    print(day, ' - ', n_entries, 'papers')
else:
    print("Failed to isolate latest day's info")

# now find the URLs to these papers for the latest day only (up to n_entries as per above)
paper_links = soup.find_all("a", {"title": "Download PDF"})[:n_entries]

# Extract the paper IDs and links
paper_ids, paper_urls = [], []
for link in paper_links:
    paper_url = "https://arxiv.org" + link["href"]
    paper_id = link["href"].split("/")[-1].split("v")[0]  # Extract the ID

    paper_ids.append(paper_id)
    paper_urls.append(paper_url)

# separately, find all titles (this is due to how the DOM is structured)
# they'll appear in the same order so order counts
paper_title_divs = soup.find_all("div", {"class": "list-title mathjax"})[:n_entries]

paper_titles = []
for title_div in paper_title_divs:
    paper_titles.append(title_div.contents[1].split('\n')[1].lstrip())

len(paper_urls), len(paper_ids), len(paper_links), len(paper_titles)

# create json linking ID and URL
paper_metadata = {paper_ids[i]: {'url': paper_urls[i]} for i in range(len(paper_ids))}
json.dump(paper_metadata, open('paper_metadata.json', 'w'))

"""## Download all papers locally"""

os.mkdir('pdfs')

i = 0
for id_, url_ in zip(paper_ids, paper_urls):
    _ = urllib_req.urlretrieve(url_, f"pdfs/{id_}.pdf")

    i += 1
    if i % 10 == 0:
        print(f"Downloaded {i} papers")

len(os.listdir('pdfs'))  # just to check count

"""## Make Gemini summarise each paper"""

# define a Pydantic model for the response
class PaperInfo(BaseModel):
    title: str = Field(description="Title of the paper")
    summary: str = Field(description="Summary of the paper, in 3 lines")
    examples: list[str] = Field(description="Relevant examples aiding comprehension, taken from the paper, if there are.")
    category: str = Field(description='Category of the paper')

# prompt
sys_instruct = cleandoc(
    """
    You are an experienced reader of academic literature and
    an expert in distilling important findings in a way that is understandable and clear.
    """)

prompt = cleandoc(
    """This is a paper on AI.
    Parse its title, summarise its results, extract examples and produce a category.
    For the summary, be concise and avoid obscure jargon.
    If there are valuable examples that aid understanding, report them in a nutshell.
    For the category, think about what the results refer to, e.g. cognitive science, medicine, foundational AI etc.
    """)

# create a dir for model responses (text)
os.mkdir('responses')

# create some dicts for data & metadata
d_response, d_usage, d_latency = {}, {}, {}

i = 0
for filename in os.listdir('pdfs')[:]:

    print(filename, i)

    # this passes the file as is to Gemini, no need to read its text content first
    file_ = client.files.upload(file=f'pdfs/{filename}')
    id_ = filename.split('.pdf')[0]

    start_time = datetime.now()
    response = client.models.generate_content(
        model=model_id,
        config=types.GenerateContentConfig(
            system_instruction=sys_instruct,
            temperature=0,                       # use greeedy decoding
            response_mime_type='application/json',
            response_schema=PaperInfo
            ),
        contents=[prompt, file_])
    end_time = datetime.now()

    # This is to handle the safety filter if triggered
    if response.prompt_feedback is not None:
        print('This paper failed with feedback: ', response.prompt_feedback)
    else:
        d_response[id_] = json.loads(response.text)
        d_usage[id_] = {
            'prompt_token_count': response.usage_metadata.prompt_token_count,
            'candidates_token_count': response.usage_metadata.candidates_token_count,
            'cached_content_token_count': response.usage_metadata.cached_content_token_count}
        d_latency[id_] = (end_time - start_time).total_seconds()

        # create file of JSON response
        json.dump(d_response[id_], open(f'responses/{id_}.json', 'w'))

        # also dump usage and latency at each execution
        json.dump(d_usage, open(f'usage.json', 'w'))
        json.dump(d_latency, open(f'latency.json', 'w'))

    i += 1

"""## Run some stats"""

# num papers summarised
print('Summaries for ', day)
print('Num papers published: ', n_entries)
print('Num papers summarised: ', len(os.listdir('responses')))
print('Median input/output tokens',
      np.percentile([d_usage[k]['prompt_token_count'] for k in d_usage.keys()], 50),
      np.percentile([d_usage[k]['candidates_token_count'] for k in d_usage.keys()], 50))
print('Median/P90 latency per paper: ',
      np.percentile([d_latency[k] for k in d_usage.keys()], 50),
      np.percentile([d_latency[k] for k in d_usage.keys()], 90))

"""## Create HTML page with all summaries"""

# this part below was contributed by Gemini after a prompt!
# prompt: Create HTML document listing all texts in folder response one after the other. Use the data in paper_metadata to create titles for each entry and an href for the link

# Load paper metadata
#paper_metadata = json.load(open('paper_metadata.json', 'r'))

# Create HTML content
html_content = """<!DOCTYPE html>
<html>
<head>
<title>Paper Summaries</title>
</head>
<body>
"""

for filename in os.listdir('responses'):
    if '.ipynb' not in filename: # it may create this folder
        paper_id = filename.split('.json')[0]
        d_paper = json.load(open(os.path.join('responses', filename), 'r'))

        if paper_id in paper_metadata:
            title = d_paper['title']
            url = paper_metadata[paper_id]['url']
            summary = d_paper['summary']
            examples = d_paper['examples']
            category = d_paper['category']
            html_content += f"<h1><a href='{url}'>{title}</a></h1>\n"
            html_content += f"<p>{summary}</p>\n<hr>\n"
            html_content += f"<p>Examples: {examples}</p>\n<hr>\n"
            html_content += f"<p>Category: {category}</p>\n<hr>\n"

html_content += """</body>
</html>"""

# Write HTML to file
with open('paper_summaries.html', 'w') as f:
    f.write(html_content)

"""## Save data to GDrive

Save zipped data, cp it to Drive timestamped with day papers refer to.
"""

# zip responses folder and all JSON and HTML files
!zip -r data.zip responses *.json *.html

# needs to mount the Drive first
drive.mount('/content/drive')

day_object = datetime.strptime(day, "%a, %d %b %Y ")
formatted_day = day_object.strftime("%Y-%m-%d")

# cp and rename
!cp *.zip /content/drive/MyDrive/
!mv /content/drive/MyDrive/data.zip /content/drive/MyDrive/{formatted_day}_data_papers.zip

# you could also send that summary HTML via email if you want
# or publish it somewhere
# I may just keep it as is for now while I test this for a few days
# currently testing how categories get created

# TODOs
# org/univ it came from - maybe if acedmic or not too
# these^ to be shown in html and put together by theme, with TOC at top
# check that zipped file is actually permanend in drive (not sure)