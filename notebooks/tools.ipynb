{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "from io import BytesIO\n",
    "import pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible tools\n",
    "\n",
    "- query_articles_list: uses the arxiv api to return a markdown table of papers with their summary and the url to access them\n",
    "- get_article: uses an article url to download the pdf and return its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_articles_list(\n",
    "    query: str, \n",
    "    sortby: str, \n",
    "    prefix: str=\"ti\",\n",
    "    start: int=0\n",
    "    ) -> str:\n",
    " \n",
    "    \"\"\"\n",
    "    Search articles on arvix according to the query value. It returns a markdown table with 20 articles and the following values:\n",
    "    - pdf: the url to the article pdf\n",
    "    - updated: the last time the article was updated\n",
    "    - published: the date when the article was published\n",
    "    - title: the article title\n",
    "    - summary: a summary of the article content\n",
    "    \n",
    "    Args:\n",
    "        query: the query used for the search\n",
    "        sortby: how to sort the results, relevance, lastUpdatedDate or submittedDate\n",
    "        prefix: how to interpret the query \n",
    "        (ti - Title, au - Author, abs - Abstract, co - Comment, jr - Journal Reference, cat - Subject Category, rn - Report Number, all - all prefixes)\n",
    "        start: the index of the ranking where the table starts, add +20 to get the next table chunk\n",
    "    \"\"\"\n",
    "\n",
    "    if sortby not in [\"relevance\", \"lastUpdatedDate\", \"submittedDate\"]:\n",
    "        sortby = \"relevance\"\n",
    "\n",
    "    if prefix not in [\"ti\", \"au\", \"abs\", \"co\", \"jr\", \"cat\", \"rn\", \"all\"]:\n",
    "        prefix = \"ti\"\n",
    "\n",
    "\n",
    "    url = f'http://export.arxiv.org/api/query?search_query={prefix}:\"{query}\"&sortBy={sortby}&sortOrder=descending&start={start}&max_results=20'\n",
    "\n",
    "    res = requests.get(url)\n",
    "\n",
    "    if not res.ok:\n",
    "        articles = 'No Results'\n",
    "    else:\n",
    "        articles = res.text\n",
    "\n",
    "    i = articles.find(\"<entry>\")\n",
    "    articles = \"<feed>\\n\" + articles[i:]\n",
    "    INVALID_CHARS = re.compile(\"[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F\\x7F]\")\n",
    "    clean = INVALID_CHARS.subn(\"\", str(articles))[0]\n",
    "    clean = clean.replace(\"&\", \"&amp;\")\n",
    "    table = pd.read_xml(io.StringIO(clean))\n",
    "    \n",
    "    table = table[[\"id\", \"updated\", \"published\", \"title\", \"summary\"]]\n",
    "    table.id = table.id.apply(lambda s: s.replace(\"/abs/\", \"/pdf/\"))\n",
    "    table = table.rename(columns={\"id\": \"pdf\"})\n",
    "    markdown = table.to_markdown(index=False)\n",
    "\n",
    "    markdown = f\"\"\" \n",
    "---{query}-{sortby}----\n",
    "{markdown}\n",
    "------------------------\n",
    "    \"\"\" \n",
    "\n",
    "    return markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "---reinforcement learning-relevance----\n",
      "| pdf                               | updated              | published            | title                                                         | summary                                                                         |\n",
      "|:----------------------------------|:---------------------|:---------------------|:--------------------------------------------------------------|:--------------------------------------------------------------------------------|\n",
      "| http://arxiv.org/pdf/2005.14419v2 | 2020-06-13T05:19:26Z | 2020-05-29T06:53:29Z | Reinforcement Learning                                        | Reinforcement learning (RL) is a general framework for adaptive control,        |\n",
      "|                                   |                      |                      |                                                               | which has proven to be efficient in many domains, e.g., board games, video      |\n",
      "|                                   |                      |                      |                                                               | games or autonomous vehicles. In such problems, an agent faces a sequential     |\n",
      "|                                   |                      |                      |                                                               | decision-making problem where, at every time step, it observes its state,       |\n",
      "|                                   |                      |                      |                                                               | performs an action, receives a reward and moves to a new state. An RL agent     |\n",
      "|                                   |                      |                      |                                                               | learns by trial and error a good policy (or controller) based on observations   |\n",
      "|                                   |                      |                      |                                                               | and numeric reward feedback on the previously performed action. In this         |\n",
      "|                                   |                      |                      |                                                               | chapter, we present the basic framework of RL and recall the two main families  |\n",
      "|                                   |                      |                      |                                                               | of approaches that have been developed to learn a good policy. The first one,   |\n",
      "|                                   |                      |                      |                                                               | which is value-based, consists in estimating the value of an optimal policy,    |\n",
      "|                                   |                      |                      |                                                               | value from which a policy can be recovered, while the other, called policy      |\n",
      "|                                   |                      |                      |                                                               | search, directly works in a policy space. Actor-critic methods can be seen as a |\n",
      "|                                   |                      |                      |                                                               | policy search technique where the policy value that is learned guides the       |\n",
      "|                                   |                      |                      |                                                               | policy improvement. Besides, we give an overview of some extensions of the      |\n",
      "|                                   |                      |                      |                                                               | standard RL framework, notably when risk-averse behavior needs to be taken into |\n",
      "|                                   |                      |                      |                                                               | account or when rewards are not available or not known.                         |\n",
      "| http://arxiv.org/pdf/2405.10369v1 | 2024-05-16T18:03:17Z | 2024-05-16T18:03:17Z | Reinforcement learning                                        | Observing celestial objects and advancing our scientific knowledge about them   |\n",
      "|                                   |                      |                      |                                                               | involves tedious planning, scheduling, data collection and data                 |\n",
      "|                                   |                      |                      |                                                               | post-processing. Many of these operational aspects of astronomy are guided and  |\n",
      "|                                   |                      |                      |                                                               | executed by expert astronomers. Reinforcement learning is a mechanism where we  |\n",
      "|                                   |                      |                      |                                                               | (as humans and astronomers) can teach agents of artificial intelligence to      |\n",
      "|                                   |                      |                      |                                                               | perform some of these tedious tasks. In this paper, we will present a state of  |\n",
      "|                                   |                      |                      |                                                               | the art overview of reinforcement learning and how it can benefit astronomy.    |\n",
      "| http://arxiv.org/pdf/2303.14623v4 | 2024-01-29T19:18:42Z | 2023-03-26T04:35:53Z | Inverse Reinforcement Learning without Reinforcement Learning | Inverse Reinforcement Learning (IRL) is a powerful set of techniques for        |\n",
      "|                                   |                      |                      |                                                               | imitation learning that aims to learn a reward function that rationalizes       |\n",
      "|                                   |                      |                      |                                                               | expert demonstrations. Unfortunately, traditional IRL methods suffer from a     |\n",
      "|                                   |                      |                      |                                                               | computational weakness: they require repeatedly solving a hard reinforcement    |\n",
      "|                                   |                      |                      |                                                               | learning (RL) problem as a subroutine. This is counter-intuitive from the       |\n",
      "|                                   |                      |                      |                                                               | viewpoint of reductions: we have reduced the easier problem of imitation        |\n",
      "|                                   |                      |                      |                                                               | learning to repeatedly solving the harder problem of RL. Another thread of work |\n",
      "|                                   |                      |                      |                                                               | has proved that access to the side-information of the distribution of states    |\n",
      "|                                   |                      |                      |                                                               | where a strong policy spends time can dramatically reduce the sample and        |\n",
      "|                                   |                      |                      |                                                               | computational complexities of solving an RL problem. In this work, we           |\n",
      "|                                   |                      |                      |                                                               | demonstrate for the first time a more informed imitation learning reduction     |\n",
      "|                                   |                      |                      |                                                               | where we utilize the state distribution of the expert to alleviate the global   |\n",
      "|                                   |                      |                      |                                                               | exploration component of the RL subroutine, providing an exponential speedup in |\n",
      "|                                   |                      |                      |                                                               | theory. In practice, we find that we are able to significantly speed up the     |\n",
      "|                                   |                      |                      |                                                               | prior art on continuous control tasks.                                          |\n",
      "| http://arxiv.org/pdf/cs/9605103v1 | 1996-05-01T00:00:00Z | 1996-05-01T00:00:00Z | Reinforcement Learning: A Survey                              | This paper surveys the field of reinforcement learning from a                   |\n",
      "|                                   |                      |                      |                                                               | computer-science perspective. It is written to be accessible to researchers     |\n",
      "|                                   |                      |                      |                                                               | familiar with machine learning. Both the historical basis of the field and a    |\n",
      "|                                   |                      |                      |                                                               | broad selection of current work are summarized. Reinforcement learning is the   |\n",
      "|                                   |                      |                      |                                                               | problem faced by an agent that learns behavior through trial-and-error          |\n",
      "|                                   |                      |                      |                                                               | interactions with a dynamic environment. The work described here has a          |\n",
      "|                                   |                      |                      |                                                               | resemblance to work in psychology, but differs considerably in the details and  |\n",
      "|                                   |                      |                      |                                                               | in the use of the word ``reinforcement.'' The paper discusses central issues of |\n",
      "|                                   |                      |                      |                                                               | reinforcement learning, including trading off exploration and exploitation,     |\n",
      "|                                   |                      |                      |                                                               | establishing the foundations of the field via Markov decision theory, learning  |\n",
      "|                                   |                      |                      |                                                               | from delayed reinforcement, constructing empirical models to accelerate         |\n",
      "|                                   |                      |                      |                                                               | learning, making use of generalization and hierarchy, and coping with hidden    |\n",
      "|                                   |                      |                      |                                                               | state. It concludes with a survey of some implemented systems and an assessment |\n",
      "|                                   |                      |                      |                                                               | of the practical utility of current methods for reinforcement learning.         |\n",
      "| http://arxiv.org/pdf/0707.3087v3  | 2009-07-22T00:58:34Z | 2007-07-20T14:51:39Z | Universal Reinforcement Learning                              | We consider an agent interacting with an unmodeled environment. At each time,   |\n",
      "|                                   |                      |                      |                                                               | the agent makes an observation, takes an action, and incurs a cost. Its actions |\n",
      "|                                   |                      |                      |                                                               | can influence future observations and costs. The goal is to minimize the        |\n",
      "|                                   |                      |                      |                                                               | long-term average cost. We propose a novel algorithm, known as the active LZ    |\n",
      "|                                   |                      |                      |                                                               | algorithm, for optimal control based on ideas from the Lempel-Ziv scheme for    |\n",
      "|                                   |                      |                      |                                                               | universal data compression and prediction. We establish that, under the active  |\n",
      "|                                   |                      |                      |                                                               | LZ algorithm, if there exists an integer $K$ such that the future is            |\n",
      "|                                   |                      |                      |                                                               | conditionally independent of the past given a window of $K$ consecutive actions |\n",
      "|                                   |                      |                      |                                                               | and observations, then the average cost converges to the optimum. Experimental  |\n",
      "|                                   |                      |                      |                                                               | results involving the game of Rock-Paper-Scissors illustrate merits of the      |\n",
      "|                                   |                      |                      |                                                               | algorithm.                                                                      |\n",
      "| http://arxiv.org/pdf/0810.3828v1  | 2008-10-21T13:38:33Z | 2008-10-21T13:38:33Z | Quantum reinforcement learning                                | The key approaches for machine learning, especially learning in unknown         |\n",
      "|                                   |                      |                      |                                                               | probabilistic environments are new representations and computation mechanisms.  |\n",
      "|                                   |                      |                      |                                                               | In this paper, a novel quantum reinforcement learning (QRL) method is proposed  |\n",
      "|                                   |                      |                      |                                                               | by combining quantum theory and reinforcement learning (RL). Inspired by the    |\n",
      "|                                   |                      |                      |                                                               | state superposition principle and quantum parallelism, a framework of value     |\n",
      "|                                   |                      |                      |                                                               | updating algorithm is introduced. The state (action) in traditional RL is       |\n",
      "|                                   |                      |                      |                                                               | identified as the eigen state (eigen action) in QRL. The state (action) set can |\n",
      "|                                   |                      |                      |                                                               | be represented with a quantum superposition state and the eigen state (eigen    |\n",
      "|                                   |                      |                      |                                                               | action) can be obtained by randomly observing the simulated quantum state       |\n",
      "|                                   |                      |                      |                                                               | according to the collapse postulate of quantum measurement. The probability of  |\n",
      "|                                   |                      |                      |                                                               | the eigen action is determined by the probability amplitude, which is           |\n",
      "|                                   |                      |                      |                                                               | parallelly updated according to rewards. Some related characteristics of QRL    |\n",
      "|                                   |                      |                      |                                                               | such as convergence, optimality and balancing between exploration and           |\n",
      "|                                   |                      |                      |                                                               | exploitation are also analyzed, which shows that this approach makes a good     |\n",
      "|                                   |                      |                      |                                                               | tradeoff between exploration and exploitation using the probability amplitude   |\n",
      "|                                   |                      |                      |                                                               | and can speed up learning through the quantum parallelism. To evaluate the      |\n",
      "|                                   |                      |                      |                                                               | performance and practicability of QRL, several simulated experiments are given  |\n",
      "|                                   |                      |                      |                                                               | and the results demonstrate the effectiveness and superiority of QRL algorithm  |\n",
      "|                                   |                      |                      |                                                               | for some complex problems. The present work is also an effective exploration on |\n",
      "|                                   |                      |                      |                                                               | the application of quantum computation to artificial intelligence.              |\n",
      "| http://arxiv.org/pdf/1303.6977v4  | 2013-06-28T11:18:26Z | 2013-03-27T20:51:33Z | ABC Reinforcement Learning                                    | This paper introduces a simple, general framework for likelihood-free           |\n",
      "|                                   |                      |                      |                                                               | Bayesian reinforcement learning, through Approximate Bayesian Computation       |\n",
      "|                                   |                      |                      |                                                               | (ABC). The main advantage is that we only require a prior distribution on a     |\n",
      "|                                   |                      |                      |                                                               | class of simulators (generative models). This is useful in domains where an     |\n",
      "|                                   |                      |                      |                                                               | analytical probabilistic model of the underlying process is too complex to      |\n",
      "|                                   |                      |                      |                                                               | formulate, but where detailed simulation models are available. ABC-RL allows    |\n",
      "|                                   |                      |                      |                                                               | the use of any Bayesian reinforcement learning technique, even in this case. In |\n",
      "|                                   |                      |                      |                                                               | addition, it can be seen as an extension of rollout algorithms to the case      |\n",
      "|                                   |                      |                      |                                                               | where we do not know what the correct model to draw rollouts from is. We        |\n",
      "|                                   |                      |                      |                                                               | experimentally demonstrate the potential of this approach in a comparison with  |\n",
      "|                                   |                      |                      |                                                               | LSPI. Finally, we introduce a theorem showing that ABC is a sound methodology   |\n",
      "|                                   |                      |                      |                                                               | in principle, even when non-sufficient statistics are used.                     |\n",
      "| http://arxiv.org/pdf/1311.2097v3  | 2014-01-23T21:18:34Z | 2013-11-08T22:25:26Z | Risk-sensitive Reinforcement Learning                         | We derive a family of risk-sensitive reinforcement learning methods for         |\n",
      "|                                   |                      |                      |                                                               | agents, who face sequential decision-making tasks in uncertain environments. By |\n",
      "|                                   |                      |                      |                                                               | applying a utility function to the temporal difference (TD) error, nonlinear    |\n",
      "|                                   |                      |                      |                                                               | transformations are effectively applied not only to the received rewards but    |\n",
      "|                                   |                      |                      |                                                               | also to the true transition probabilities of the underlying Markov decision     |\n",
      "|                                   |                      |                      |                                                               | process. When appropriate utility functions are chosen, the agents' behaviors   |\n",
      "|                                   |                      |                      |                                                               | express key features of human behavior as predicted by prospect theory          |\n",
      "|                                   |                      |                      |                                                               | (Kahneman and Tversky, 1979), for example different risk-preferences for gains  |\n",
      "|                                   |                      |                      |                                                               | and losses as well as the shape of subjective probability curves. We derive a   |\n",
      "|                                   |                      |                      |                                                               | risk-sensitive Q-learning algorithm, which is necessary for modeling human      |\n",
      "|                                   |                      |                      |                                                               | behavior when transition probabilities are unknown, and prove its convergence.  |\n",
      "|                                   |                      |                      |                                                               | As a proof of principle for the applicability of the new framework we apply it  |\n",
      "|                                   |                      |                      |                                                               | to quantify human behavior in a sequential investment task. We find, that the   |\n",
      "|                                   |                      |                      |                                                               | risk-sensitive variant provides a significantly better fit to the behavioral    |\n",
      "|                                   |                      |                      |                                                               | data and that it leads to an interpretation of the subject's responses which is |\n",
      "|                                   |                      |                      |                                                               | indeed consistent with prospect theory. The analysis of simultaneously measured |\n",
      "|                                   |                      |                      |                                                               | fMRI signals show a significant correlation of the risk-sensitive TD error with |\n",
      "|                                   |                      |                      |                                                               | BOLD signal change in the ventral striatum. In addition we find a significant   |\n",
      "|                                   |                      |                      |                                                               | correlation of the risk-sensitive Q-values with neural activity in the          |\n",
      "|                                   |                      |                      |                                                               | striatum, cingulate cortex and insula, which is not present if standard         |\n",
      "|                                   |                      |                      |                                                               | Q-values are used.                                                              |\n",
      "| http://arxiv.org/pdf/1606.02396v1 | 2016-06-08T04:48:49Z | 2016-06-08T04:48:49Z | Deep Successor Reinforcement Learning                         | Learning robust value functions given raw observations and rewards is now       |\n",
      "|                                   |                      |                      |                                                               | possible with model-free and model-based deep reinforcement learning            |\n",
      "|                                   |                      |                      |                                                               | algorithms. There is a third alternative, called Successor Representations      |\n",
      "|                                   |                      |                      |                                                               | (SR), which decomposes the value function into two components -- a reward       |\n",
      "|                                   |                      |                      |                                                               | predictor and a successor map. The successor map represents the expected future |\n",
      "|                                   |                      |                      |                                                               | state occupancy from any given state and the reward predictor maps states to    |\n",
      "|                                   |                      |                      |                                                               | scalar rewards. The value function of a state can be computed as the inner      |\n",
      "|                                   |                      |                      |                                                               | product between the successor map and the reward weights. In this paper, we     |\n",
      "|                                   |                      |                      |                                                               | present DSR, which generalizes SR within an end-to-end deep reinforcement       |\n",
      "|                                   |                      |                      |                                                               | learning framework. DSR has several appealing properties including: increased   |\n",
      "|                                   |                      |                      |                                                               | sensitivity to distal reward changes due to factorization of reward and world   |\n",
      "|                                   |                      |                      |                                                               | dynamics, and the ability to extract bottleneck states (subgoals) given         |\n",
      "|                                   |                      |                      |                                                               | successor maps trained under a random policy. We show the efficacy of our       |\n",
      "|                                   |                      |                      |                                                               | approach on two diverse environments given raw pixel observations -- simple     |\n",
      "|                                   |                      |                      |                                                               | grid-world domains (MazeBase) and the Doom game engine.                         |\n",
      "| http://arxiv.org/pdf/1606.03137v4 | 2024-02-17T16:13:12Z | 2016-06-09T22:39:54Z | Cooperative Inverse Reinforcement Learning                    | For an autonomous system to be helpful to humans and to pose no unwarranted     |\n",
      "|                                   |                      |                      |                                                               | risks, it needs to align its values with those of the humans in its environment |\n",
      "|                                   |                      |                      |                                                               | in such a way that its actions contribute to the maximization of value for the  |\n",
      "|                                   |                      |                      |                                                               | humans. We propose a formal definition of the value alignment problem as        |\n",
      "|                                   |                      |                      |                                                               | cooperative inverse reinforcement learning (CIRL). A CIRL problem is a          |\n",
      "|                                   |                      |                      |                                                               | cooperative, partial-information game with two agents, human and robot; both    |\n",
      "|                                   |                      |                      |                                                               | are rewarded according to the human's reward function, but the robot does not   |\n",
      "|                                   |                      |                      |                                                               | initially know what this is. In contrast to classical IRL, where the human is   |\n",
      "|                                   |                      |                      |                                                               | assumed to act optimally in isolation, optimal CIRL solutions produce behaviors |\n",
      "|                                   |                      |                      |                                                               | such as active teaching, active learning, and communicative actions that are    |\n",
      "|                                   |                      |                      |                                                               | more effective in achieving value alignment. We show that computing optimal     |\n",
      "|                                   |                      |                      |                                                               | joint policies in CIRL games can be reduced to solving a POMDP, prove that      |\n",
      "|                                   |                      |                      |                                                               | optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL   |\n",
      "|                                   |                      |                      |                                                               | algorithm.                                                                      |\n",
      "| http://arxiv.org/pdf/1611.00862v1 | 2016-11-03T02:28:53Z | 2016-11-03T02:28:53Z | Quantile Reinforcement Learning                               | In reinforcement learning, the standard criterion to evaluate policies in a     |\n",
      "|                                   |                      |                      |                                                               | state is the expectation of (discounted) sum of rewards. However, this          |\n",
      "|                                   |                      |                      |                                                               | criterion may not always be suitable, we consider an alternative criterion      |\n",
      "|                                   |                      |                      |                                                               | based on the notion of quantiles. In the case of episodic reinforcement         |\n",
      "|                                   |                      |                      |                                                               | learning problems, we propose an algorithm based on stochastic approximation    |\n",
      "|                                   |                      |                      |                                                               | with two timescales. We evaluate our proposition on a simple model of the TV    |\n",
      "|                                   |                      |                      |                                                               | show, Who wants to be a millionaire.                                            |\n",
      "| http://arxiv.org/pdf/1611.03071v4 | 2017-08-06T00:12:49Z | 2016-11-09T20:19:45Z | Fairness in Reinforcement Learning                            | We initiate the study of fairness in reinforcement learning, where the          |\n",
      "|                                   |                      |                      |                                                               | actions of a learning algorithm may affect its environment and future rewards.  |\n",
      "|                                   |                      |                      |                                                               | Our fairness constraint requires that an algorithm never prefers one action     |\n",
      "|                                   |                      |                      |                                                               | over another if the long-term (discounted) reward of choosing the latter action |\n",
      "|                                   |                      |                      |                                                               | is higher. Our first result is negative: despite the fact that fairness is      |\n",
      "|                                   |                      |                      |                                                               | consistent with the optimal policy, any learning algorithm satisfying fairness  |\n",
      "|                                   |                      |                      |                                                               | must take time exponential in the number of states to achieve non-trivial       |\n",
      "|                                   |                      |                      |                                                               | approximation to the optimal policy. We then provide a provably fair polynomial |\n",
      "|                                   |                      |                      |                                                               | time algorithm under an approximate notion of fairness, thus establishing an    |\n",
      "|                                   |                      |                      |                                                               | exponential gap between exact and approximate fairness                          |\n",
      "| http://arxiv.org/pdf/1611.05763v3 | 2017-01-23T12:38:24Z | 2016-11-17T16:29:11Z | Learning to reinforcement learn                               | In recent years deep reinforcement learning (RL) systems have attained          |\n",
      "|                                   |                      |                      |                                                               | superhuman performance in a number of challenging task domains. However, a      |\n",
      "|                                   |                      |                      |                                                               | major limitation of such applications is their demand for massive amounts of    |\n",
      "|                                   |                      |                      |                                                               | training data. A critical present objective is thus to develop deep RL methods  |\n",
      "|                                   |                      |                      |                                                               | that can adapt rapidly to new tasks. In the present work we introduce a novel   |\n",
      "|                                   |                      |                      |                                                               | approach to this challenge, which we refer to as deep meta-reinforcement        |\n",
      "|                                   |                      |                      |                                                               | learning. Previous work has shown that recurrent networks can support           |\n",
      "|                                   |                      |                      |                                                               | meta-learning in a fully supervised context. We extend this approach to the RL  |\n",
      "|                                   |                      |                      |                                                               | setting. What emerges is a system that is trained using one RL algorithm, but   |\n",
      "|                                   |                      |                      |                                                               | whose recurrent dynamics implement a second, quite separate RL procedure. This  |\n",
      "|                                   |                      |                      |                                                               | second, learned RL algorithm can differ from the original one in arbitrary      |\n",
      "|                                   |                      |                      |                                                               | ways. Importantly, because it is learned, it is configured to exploit structure |\n",
      "|                                   |                      |                      |                                                               | in the training domain. We unpack these points in a series of seven             |\n",
      "|                                   |                      |                      |                                                               | proof-of-concept experiments, each of which examines a key aspect of deep       |\n",
      "|                                   |                      |                      |                                                               | meta-RL. We consider prospects for extending and scaling up the approach, and   |\n",
      "|                                   |                      |                      |                                                               | also point out some potentially important implications for neuroscience.        |\n",
      "| http://arxiv.org/pdf/1611.08944v1 | 2016-11-28T00:36:40Z | 2016-11-28T00:36:40Z | Nonparametric General Reinforcement Learning                  | Reinforcement learning (RL) problems are often phrased in terms of Markov       |\n",
      "|                                   |                      |                      |                                                               | decision processes (MDPs). In this thesis we go beyond MDPs and consider RL in  |\n",
      "|                                   |                      |                      |                                                               | environments that are non-Markovian, non-ergodic and only partially observable. |\n",
      "|                                   |                      |                      |                                                               | Our focus is not on practical algorithms, but rather on the fundamental         |\n",
      "|                                   |                      |                      |                                                               | underlying problems: How do we balance exploration and exploitation? How do we  |\n",
      "|                                   |                      |                      |                                                               | explore optimally? When is an agent optimal? We follow the nonparametric        |\n",
      "|                                   |                      |                      |                                                               | realizable paradigm.                                                            |\n",
      "|                                   |                      |                      |                                                               |   We establish negative results on Bayesian RL agents, in particular AIXI. We   |\n",
      "|                                   |                      |                      |                                                               | show that unlucky or adversarial choices of the prior cause the agent to        |\n",
      "|                                   |                      |                      |                                                               | misbehave drastically. Therefore Legg-Hutter intelligence and balanced Pareto   |\n",
      "|                                   |                      |                      |                                                               | optimality, which depend crucially on the choice of the prior, are entirely     |\n",
      "|                                   |                      |                      |                                                               | subjective. Moreover, in the class of all computable environments every policy  |\n",
      "|                                   |                      |                      |                                                               | is Pareto optimal. This undermines all existing optimality properties for AIXI. |\n",
      "|                                   |                      |                      |                                                               | However, there are Bayesian approaches to general RL that satisfy objective     |\n",
      "|                                   |                      |                      |                                                               | optimality guarantees: We prove that Thompson sampling is asymptotically        |\n",
      "|                                   |                      |                      |                                                               | optimal in stochastic environments in the sense that its value converges to the |\n",
      "|                                   |                      |                      |                                                               | value of the optimal policy. We connect asymptotic optimality to regret given a |\n",
      "|                                   |                      |                      |                                                               | recoverability assumption on the environment that allows the agent to recover   |\n",
      "|                                   |                      |                      |                                                               | from mistakes. Hence Thompson sampling achieves sublinear regret in these       |\n",
      "|                                   |                      |                      |                                                               | environments.                                                                   |\n",
      "|                                   |                      |                      |                                                               |   Our results culminate in a formal solution to the grain of truth problem: A   |\n",
      "|                                   |                      |                      |                                                               | Bayesian agent acting in a multi-agent environment learns to predict the other  |\n",
      "|                                   |                      |                      |                                                               | agents' policies if its prior assigns positive probability to them (the prior   |\n",
      "|                                   |                      |                      |                                                               | contains a grain of truth). We construct a large but limit computable class     |\n",
      "|                                   |                      |                      |                                                               | containing a grain of truth and show that agents based on Thompson sampling     |\n",
      "|                                   |                      |                      |                                                               | over this class converge to play Nash equilibria in arbitrary unknown           |\n",
      "|                                   |                      |                      |                                                               | computable multi-agent environments.                                            |\n",
      "| http://arxiv.org/pdf/1701.08810v3 | 2017-11-14T21:08:17Z | 2017-01-30T20:13:17Z | Reinforcement Learning Algorithm Selection                    | This paper formalises the problem of online algorithm selection in the          |\n",
      "|                                   |                      |                      |                                                               | context of Reinforcement Learning. The setup is as follows: given an episodic   |\n",
      "|                                   |                      |                      |                                                               | task and a finite number of off-policy RL algorithms, a meta-algorithm has to   |\n",
      "|                                   |                      |                      |                                                               | decide which RL algorithm is in control during the next episode so as to        |\n",
      "|                                   |                      |                      |                                                               | maximize the expected return. The article presents a novel meta-algorithm,      |\n",
      "|                                   |                      |                      |                                                               | called Epochal Stochastic Bandit Algorithm Selection (ESBAS). Its principle is  |\n",
      "|                                   |                      |                      |                                                               | to freeze the policy updates at each epoch, and to leave a rebooted stochastic  |\n",
      "|                                   |                      |                      |                                                               | bandit in charge of the algorithm selection. Under some assumptions, a thorough |\n",
      "|                                   |                      |                      |                                                               | theoretical analysis demonstrates its near-optimality considering the           |\n",
      "|                                   |                      |                      |                                                               | structural sampling budget limitations. ESBAS is first empirically evaluated on |\n",
      "|                                   |                      |                      |                                                               | a dialogue task where it is shown to outperform each individual algorithm in    |\n",
      "|                                   |                      |                      |                                                               | most configurations. ESBAS is then adapted to a true online setting where       |\n",
      "|                                   |                      |                      |                                                               | algorithms update their policies after each transition, which we call SSBAS.    |\n",
      "|                                   |                      |                      |                                                               | SSBAS is evaluated on a fruit collection task where it is shown to adapt the    |\n",
      "|                                   |                      |                      |                                                               | stepsize parameter more efficiently than the classical hyperbolic decay, and on |\n",
      "|                                   |                      |                      |                                                               | an Atari game, where it improves the performance by a wide margin.              |\n",
      "| http://arxiv.org/pdf/1703.02702v1 | 2017-03-08T04:58:51Z | 2017-03-08T04:58:51Z | Robust Adversarial Reinforcement Learning                     | Deep neural networks coupled with fast simulation and improved computation      |\n",
      "|                                   |                      |                      |                                                               | have led to recent successes in the field of reinforcement learning (RL).       |\n",
      "|                                   |                      |                      |                                                               | However, most current RL-based approaches fail to generalize since: (a) the gap |\n",
      "|                                   |                      |                      |                                                               | between simulation and real world is so large that policy-learning approaches   |\n",
      "|                                   |                      |                      |                                                               | fail to transfer; (b) even if policy learning is done in real world, the data   |\n",
      "|                                   |                      |                      |                                                               | scarcity leads to failed generalization from training to test scenarios (e.g.,  |\n",
      "|                                   |                      |                      |                                                               | due to different friction or object masses). Inspired from H-infinity control   |\n",
      "|                                   |                      |                      |                                                               | methods, we note that both modeling errors and differences in training and test |\n",
      "|                                   |                      |                      |                                                               | scenarios can be viewed as extra forces/disturbances in the system. This paper  |\n",
      "|                                   |                      |                      |                                                               | proposes the idea of robust adversarial reinforcement learning (RARL), where we |\n",
      "|                                   |                      |                      |                                                               | train an agent to operate in the presence of a destabilizing adversary that     |\n",
      "|                                   |                      |                      |                                                               | applies disturbance forces to the system. The jointly trained adversary is      |\n",
      "|                                   |                      |                      |                                                               | reinforced -- that is, it learns an optimal destabilization policy. We          |\n",
      "|                                   |                      |                      |                                                               | formulate the policy learning as a zero-sum, minimax objective function.        |\n",
      "|                                   |                      |                      |                                                               | Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah,  |\n",
      "|                                   |                      |                      |                                                               | Swimmer, Hopper and Walker2d) conclusively demonstrate that our method (a)      |\n",
      "|                                   |                      |                      |                                                               | improves training stability; (b) is robust to differences in training/test      |\n",
      "|                                   |                      |                      |                                                               | conditions; and c) outperform the baseline even in the absence of the           |\n",
      "|                                   |                      |                      |                                                               | adversary.                                                                      |\n",
      "| http://arxiv.org/pdf/1704.00756v2 | 2017-11-14T20:35:27Z | 2017-04-03T18:37:12Z | Multi-Advisor Reinforcement Learning                          | We consider tackling a single-agent RL problem by distributing it to $n$        |\n",
      "|                                   |                      |                      |                                                               | learners. These learners, called advisors, endeavour to solve the problem from  |\n",
      "|                                   |                      |                      |                                                               | a different focus. Their advice, taking the form of action values, is then      |\n",
      "|                                   |                      |                      |                                                               | communicated to an aggregator, which is in control of the system. We show that  |\n",
      "|                                   |                      |                      |                                                               | the local planning method for the advisors is critical and that none of the     |\n",
      "|                                   |                      |                      |                                                               | ones found in the literature is flawless: the egocentric planning overestimates |\n",
      "|                                   |                      |                      |                                                               | values of states where the other advisors disagree, and the agnostic planning   |\n",
      "|                                   |                      |                      |                                                               | is inefficient around danger zones. We introduce a novel approach called        |\n",
      "|                                   |                      |                      |                                                               | empathic and discuss its theoretical aspects. We empirically examine and        |\n",
      "|                                   |                      |                      |                                                               | validate our theoretical findings on a fruit collection task.                   |\n",
      "| http://arxiv.org/pdf/1705.05427v3 | 2017-11-04T00:38:19Z | 2017-05-15T20:06:35Z | Repeated Inverse Reinforcement Learning                       | We introduce a novel repeated Inverse Reinforcement Learning problem: the       |\n",
      "|                                   |                      |                      |                                                               | agent has to act on behalf of a human in a sequence of tasks and wishes to      |\n",
      "|                                   |                      |                      |                                                               | minimize the number of tasks that it surprises the human by acting suboptimally |\n",
      "|                                   |                      |                      |                                                               | with respect to how the human would have acted. Each time the human is          |\n",
      "|                                   |                      |                      |                                                               | surprised, the agent is provided a demonstration of the desired behavior by the |\n",
      "|                                   |                      |                      |                                                               | human. We formalize this problem, including how the sequence of tasks is        |\n",
      "|                                   |                      |                      |                                                               | chosen, in a few different ways and provide some foundational results.          |\n",
      "| http://arxiv.org/pdf/1801.08099v8 | 2019-02-16T18:11:58Z | 2018-01-24T17:50:30Z | Logically-Constrained Reinforcement Learning                  | We present the first model-free Reinforcement Learning (RL) algorithm to        |\n",
      "|                                   |                      |                      |                                                               | synthesise policies for an unknown Markov Decision Process (MDP), such that a   |\n",
      "|                                   |                      |                      |                                                               | linear time property is satisfied. The given temporal property is converted     |\n",
      "|                                   |                      |                      |                                                               | into a Limit Deterministic Buchi Automaton (LDBA) and a robust reward function  |\n",
      "|                                   |                      |                      |                                                               | is defined over the state-action pairs of the MDP according to the resulting    |\n",
      "|                                   |                      |                      |                                                               | LDBA. With this reward function, the policy synthesis procedure is              |\n",
      "|                                   |                      |                      |                                                               | \"constrained\" by the given specification. These constraints guide the MDP       |\n",
      "|                                   |                      |                      |                                                               | exploration so as to minimize the solution time by only considering the portion |\n",
      "|                                   |                      |                      |                                                               | of the MDP that is relevant to satisfaction of the LTL property. This improves  |\n",
      "|                                   |                      |                      |                                                               | performance and scalability of the proposed method by avoiding an exhaustive    |\n",
      "|                                   |                      |                      |                                                               | update over the whole state space while the efficiency of standard methods such |\n",
      "|                                   |                      |                      |                                                               | as dynamic programming is hindered by excessive memory requirements, caused by  |\n",
      "|                                   |                      |                      |                                                               | the need to store a full-model in memory. Additionally, we show that the RL     |\n",
      "|                                   |                      |                      |                                                               | procedure sets up a local value iteration method to efficiently calculate the   |\n",
      "|                                   |                      |                      |                                                               | maximum probability of satisfying the given property, at any given state of the |\n",
      "|                                   |                      |                      |                                                               | MDP. We prove that our algorithm is guaranteed to find a policy whose traces    |\n",
      "|                                   |                      |                      |                                                               | probabilistically satisfy the LTL property if such a policy exists, and         |\n",
      "|                                   |                      |                      |                                                               | additionally we show that our method produces reasonable control policies even  |\n",
      "|                                   |                      |                      |                                                               | when the LTL property cannot be satisfied. The performance of the algorithm is  |\n",
      "|                                   |                      |                      |                                                               | evaluated via a set of numerical examples. We observe an improvement of one     |\n",
      "|                                   |                      |                      |                                                               | order of magnitude in the number of iterations required for the synthesis       |\n",
      "|                                   |                      |                      |                                                               | compared to existing approaches.                                                |\n",
      "| http://arxiv.org/pdf/1805.09801v1 | 2018-05-24T17:45:11Z | 2018-05-24T17:45:11Z | Meta-Gradient Reinforcement Learning                          | The goal of reinforcement learning algorithms is to estimate and/or optimise    |\n",
      "|                                   |                      |                      |                                                               | the value function. However, unlike supervised learning, no teacher or oracle   |\n",
      "|                                   |                      |                      |                                                               | is available to provide the true value function. Instead, the majority of       |\n",
      "|                                   |                      |                      |                                                               | reinforcement learning algorithms estimate and/or optimise a proxy for the      |\n",
      "|                                   |                      |                      |                                                               | value function. This proxy is typically based on a sampled and bootstrapped     |\n",
      "|                                   |                      |                      |                                                               | approximation to the true value function, known as a return. The particular     |\n",
      "|                                   |                      |                      |                                                               | choice of return is one of the chief components determining the nature of the   |\n",
      "|                                   |                      |                      |                                                               | algorithm: the rate at which future rewards are discounted; when and how values |\n",
      "|                                   |                      |                      |                                                               | should be bootstrapped; or even the nature of the rewards themselves. It is     |\n",
      "|                                   |                      |                      |                                                               | well-known that these decisions are crucial to the overall success of RL        |\n",
      "|                                   |                      |                      |                                                               | algorithms. We discuss a gradient-based meta-learning algorithm that is able to |\n",
      "|                                   |                      |                      |                                                               | adapt the nature of the return, online, whilst interacting and learning from    |\n",
      "|                                   |                      |                      |                                                               | the environment. When applied to 57 games on the Atari 2600 environment over    |\n",
      "|                                   |                      |                      |                                                               | 200 million frames, our algorithm achieved a new state-of-the-art performance.  |\n",
      "------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(query_articles_list(\"reinforcement learning\",\"relevance\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------http://arxiv.org/pdf/1611.05763v3------------\n",
      "LEARNING TO REINFORCEMENT LEARN\n",
      "JX Wang1, Z Kurth-Nelson1, D Tirumala1, H Soyer1, JZ Leibo1,\n",
      "R Munos1, C Blundell1, D Kumaran1,3, M Botvinick1,2\n",
      "1DeepMind, London, UK\n",
      "2Gatsby Computational Neuroscience Unit, UCL, London, UK\n",
      "3Institute of Cognitive Neuroscience, UCL, London, UK\n",
      "{wangjane, zebk, dhruvat, soyer, jzl, munos, cblundell,\n",
      "dkumaran, botvinick} @google.com\n",
      "ABSTRACT\n",
      "In recent years deep reinforcement learning (RL) systems have attained superhuman\n",
      "performance in a number of challenging task domains. However, a major limitation\n",
      "of such applications is their demand for massive amounts of training data. A critical\n",
      "present objective is thus to develop deep RL methods that can adapt rapidly to new\n",
      "tasks. In the present work we introduce a novel approach to this challenge, which\n",
      "we refer to as deep meta-reinforcement learning. Previous work has shown that\n",
      "recurrent networks can support meta-learning in a fully supervised context. We\n",
      "extend this approach to the RL setting. What emerges is a system that is trained\n",
      "using one RL algorithm, but whose recurrent dynamics implement a second, quite\n",
      "separate RL procedure. This second, learned RL algorithm can differ from the\n",
      "original one in arbitrary ways. Importantly, because it is learned, it is conﬁgured\n",
      "to exploit structure in the training domain. We unpack these points in a series of\n",
      "seven proof-of-concept experiments, each of which examines a key aspect of deep\n",
      "meta-RL. We consider prospects for extending and scaling up the approach, and\n",
      "also point out some potentially important implications for neuroscience.\n",
      "1\n",
      "INTRODUCTION\n",
      "Recent advances have allowed long-standing methods for reinforcement learning (RL) to be newly\n",
      "extended to such complex and large-scale task environments as Atari (Mnih et al., 2015) and Go\n",
      "(Silver et al., 2016). The key enabling breakthrough has been the development of techniques allowing\n",
      "the stable integration of RL with non-linear function approximation through deep learning (LeCun\n",
      "et al., 2015; Mnih et al., 2015). The resulting deep RL methods are attaining human- and often\n",
      "superhuman-level performance in an expanding list of domains (Jaderberg et al., 2016; Mnih et al.,\n",
      "2015; Silver et al., 2016). However, there are at least two aspects of human performance that they\n",
      "starkly lack. First, deep RL typically requires a massive volume of training data, whereas human\n",
      "learners can attain reasonable performance on any of a wide range of tasks with comparatively little\n",
      "experience. Second, deep RL systems typically specialize on one restricted task domain, whereas\n",
      "human learners can ﬂexibly adapt to changing task conditions. Recent critiques (e.g., Lake et al.,\n",
      "2016) have invoked these differences as posing a direct challenge to current deep RL research.\n",
      "In the present work, we outline a framework for meeting these challenges, which we refer to as\n",
      "deep meta-reinforcement learning, a label that is intended to both link it with and distinguish it\n",
      "from previous work employing the term “meta-reinforcement learning” (e.g. Schmidhuber et al.,\n",
      "1996; Schweighofer and Doya, 2003, discussed later). The key concept is to use standard deep RL\n",
      "techniques to train a recurrent neural network in such a way that the recurrent network comes to\n",
      "implement its own, free-standing RL procedure. As we shall illustrate, under the right circumstances,\n",
      "the secondary learned RL procedure can display an adaptiveness and sample efﬁciency that the\n",
      "original RL procedure lacks.\n",
      "The following sections review previous work employing recurrent neural networks in the context of\n",
      "meta-learning and describe the general approach for extending such methods to the RL setting. We\n",
      "1\n",
      "arXiv:1611.05763v3  [cs.LG]  23 Jan 2017\n",
      "\fthen present seven proof-of-concept experiments, each of which highlights an important ramiﬁcation\n",
      "of the deep meta-RL setup by characterizing agent performance in light of this framework. We close\n",
      "with a discussion of key challenges for next-step research, as well as some potential implications for\n",
      "neuroscience.\n",
      "2\n",
      "METHODS\n",
      "2.1\n",
      "BACKGROUND: META-LEARNING IN RECURRENT NEURAL NETWORKS\n",
      "Flexible, data-efﬁcient learning naturally requires the operation of prior biases. In general terms,\n",
      "such biases can derive from two sources; they can either be engineered into the learning system (as,\n",
      "for example, in convolutional networks), or they can themselves be acquired through learning. The\n",
      "second case has been explored in the machine learning literature under the rubric of meta-learning\n",
      "(Schmidhuber et al., 1996; Thrun and Pratt, 1998).\n",
      "In one standard setup, the learning agent is confronted with a series of tasks that differ from one\n",
      "another but also share some underlying set of regularities. Meta-learning is then deﬁned as an\n",
      "effect whereby the agent improves its performance in each new task more rapidly, on average, than\n",
      "in past tasks (Thrun and Pratt, 1998). At an architectural level, meta-learning has generally been\n",
      "conceptualized as involving two learning systems: one lower-level system that learns relatively\n",
      "quickly, and which is primarily responsible for adapting to each new task; and a slower higher-level\n",
      "system that works across tasks to tune and improve the lower-level system.\n",
      "A variety of methods have been pursued to implement this basic meta-learning setup, both within\n",
      "the deep learning community and beyond (Thrun and Pratt, 1998). Of particular relevance here is\n",
      "an approach introduced by Hochreiter and colleagues (Hochreiter et al., 2001), in which a recurrent\n",
      "neural network is trained on a series of interrelated tasks using standard backpropagation. A critical\n",
      "aspect of their setup is that the network receives, on each step within a task, an auxiliary input\n",
      "indicating the target output for the preceding step. For example, in a regression task, on each step\n",
      "the network receives as input an x value for which it is desired to output the corresponding y, but the\n",
      "network also receives an input disclosing the target y value for the preceding step (see Hochreiter\n",
      "et al., 2001; Santoro et al., 2016). In this scenario, a different function is used to generate the data\n",
      "in each training episode, but if the functions are all drawn from a single parametric family, then the\n",
      "system gradually tunes into this consistent structure, converging on accurate outputs more and more\n",
      "rapidly across episodes.\n",
      "One interesting aspect of Hochreiter’s method is that the process that underlies learning within each\n",
      "new task inheres entirely in the dynamics of the recurrent network, rather than in the backpropagation\n",
      "procedure used to tune that network’s weights. Indeed, after an initial training period, the network\n",
      "can improve its performance on new tasks even if the weights are held constant (see also Cotter\n",
      "and Conwell, 1990; Prokhorov et al., 2002; Younger et al., 1999). A second important aspect of the\n",
      "approach is that the learning procedure implemented in the recurrent network is ﬁt to the structure\n",
      "that spans the family of tasks on which the network is trained, embedding biases that allow it to learn\n",
      "efﬁciently when dealing with tasks from that family.\n",
      "2.2\n",
      "DEEP META-RL: DEFINITION AND KEY FEATURES\n",
      "Importantly, Hochreiter’s original work (Hochreiter et al., 2001), as well as its subsequent extensions\n",
      "(Cotter and Conwell, 1990; Prokhorov et al., 2002; Santoro et al., 2016; Younger et al., 1999) only\n",
      "addressed supervised learning (i.e. the auxiliary input provided on each step explicitly indicated the\n",
      "target output on the previous step, and the network was trained using explicit targets). In the present\n",
      "work we consider the implications of applying the same approach in the context of reinforcement\n",
      "learning. Here, the tasks that make up the training series are interrelated RL problems, for example,\n",
      "a series of bandit problems varying only in their parameterization. Rather than presenting target\n",
      "outputs as auxiliary inputs, the agent receives inputs indicating the action output on the previous step\n",
      "and, critically, the quantity of reward resulting from that action. The same reward information is fed\n",
      "in parallel to a deep RL procedure, which tunes the weights of the recurrent network.\n",
      "It is this setup, as well as its result, that we refer to as deep meta-RL (although from here on, for\n",
      "brevity, we will often simply call it meta-RL, with apologies to authors who have used that term\n",
      "2\n",
      "\fpreviously). As in the supervised case, when the approach is successful, the dynamics of the recurrent\n",
      "network come to implement a learning algorithm entirely separate from the one used to train the\n",
      "network weights. Once again, after sufﬁcient training, learning can occur within each task even if the\n",
      "weights are held constant. However, here the procedure the recurrent network implements is itself a\n",
      "full-ﬂedged reinforcement learning algorithm, which negotiates the exploration-exploitation tradeoff\n",
      "and improves the agent’s policy based on reward outcomes. A key point, which we will emphasize in\n",
      "what follows, is that this learned RL procedure can differ starkly from the algorithm used to train the\n",
      "network’s weights. In particular, its policy update procedure (including features such as the effective\n",
      "learning rate of that procedure), can differ dramatically from those involved in tuning the network\n",
      "weights, and the learned RL procedure can implement its own approach to exploration. Critically, as\n",
      "in the supervised case, the learned RL procedure will be ﬁt to the statistics spanning the multi-task\n",
      "environment, allowing it to adapt rapidly to new task instances.\n",
      "2.3\n",
      "FORMALISM\n",
      "Let us write as D a distribution (the prior) over Markov Decision Processes (MDPs). We want to\n",
      "demonstrate that meta-RL is able to learn a prior-dependent RL algorithm, in the sense that it will\n",
      "perform well on average on MDPs drawn from D or slight modiﬁcations of D. An appropriately\n",
      "structured agent, embedding a recurrent neural network, is trained by interacting with a sequence of\n",
      "MDP environments (also called tasks) through episodes. At the start of a new episode, a new MDP\n",
      "task m ∼D and an initial state for this task are sampled, and the internal state of the agent (i.e., the\n",
      "pattern of activation over its recurrent units) is reset. The agent then executes its action-selection\n",
      "strategy in this environment for a certain number of discrete time-steps. At each step t an action\n",
      "at ∈A is executed as a function of the whole history Ht = {x0, a0, r0, . . . , xt−1, at−1, rt−1, xt}\n",
      "of the agent interacting in the MDP m during the current episode (set of states {xs}0≤s≤t, actions\n",
      "{as}0≤s<t, and rewards {rs}0≤s<t observed since the beginning of the episode, when the recurrent\n",
      "unit was reset). The network weights are trained to maximize the sum of observed rewards over all\n",
      "steps and episodes.\n",
      "After training, the agent’s policy is ﬁxed (i.e. the weights are frozen, but the activations are changing\n",
      "due to input from the environment and the hidden state of the recurrent layer), and it is evaluated\n",
      "on a set of MDPs that are drawn either from the same distribution D or slight modiﬁcations of that\n",
      "distribution (to test the generalization capacity of the agent). The internal state is reset at the beginning\n",
      "of the evaluation of any new episode. Since the policy learned by the agent is history-dependent (as it\n",
      "makes uses of a recurrent network), when exposed to any new MDP environment, it is able to adapt\n",
      "and deploy a strategy that optimizes rewards for that task.\n",
      "3\n",
      "EXPERIMENTS\n",
      "In order to evaluate the approach to learning that we have just described, we conducted a series of\n",
      "six proof-of-concept experiments, which we present here along with a seventh experiment originally\n",
      "reported in a related paper (Mirowski et al., 2016). One particular point of interest in these experiments\n",
      "was to see whether meta-RL could be used to learn an adaptive balance between exploration and\n",
      "exploitation, as demanded of any fully-ﬂedged RL procedure. A second and still more important\n",
      "focus was on the question of whether meta-RL can give rise to learning that gains efﬁciency by\n",
      "capitalizing on task structure.\n",
      "In order to examine these questions, we performed four experiments focusing on bandit tasks and two\n",
      "additional experiments focusing on Markov decision problems. All of our experiments (as well as the\n",
      "additional experiment we report) employ a common set of methods, with minor implementational\n",
      "variations. In all experiments, the agent architecture centers on a recurrent neural network (LSTM;\n",
      "Hochreiter and Schmidhuber, 1997) feeding into a soft-max output representing discrete actions.\n",
      "As detailed below, the parameters of this network core, as well as some other architectural details,\n",
      "varied across experiments (see Figure 1 and Table 1). However, it is important to emphasize that\n",
      "comparisons between speciﬁc architectures are outside the scope of this paper. Our main aim is to\n",
      "illustrate and validate the meta-RL framework in a more general way. To this end, all experiments\n",
      "used the high-level task setup previously described: Both training and testing were organized into\n",
      "ﬁxed-length episodes, each involving a task randomly sampled from a predetermined task distribution,\n",
      "with the LSTM hidden state initialized at the beginning of each episode. Task-speciﬁc inputs and\n",
      "3\n",
      "\fParameter\n",
      "Exps. 1 & 2\n",
      "Exp. 3\n",
      "Exp. 4\n",
      "Exp. 5\n",
      "Exp. 6\n",
      "No. threads\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "32\n",
      "No. LSTMs\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "No. hiddens\n",
      "48\n",
      "48\n",
      "48\n",
      "48\n",
      "256/64\n",
      "Steps unrolled\n",
      "100\n",
      "5\n",
      "150\n",
      "20\n",
      "100\n",
      "βe\n",
      "annealed\n",
      "annealed\n",
      "annealed\n",
      "0.05\n",
      "0.001\n",
      "βv\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.4\n",
      "Learning rate\n",
      "tuned\n",
      "0.001\n",
      "0.001\n",
      "tuned\n",
      "tuned\n",
      "Discount factor\n",
      "tuned\n",
      "0.8\n",
      "0.8\n",
      "tuned\n",
      "tuned\n",
      "Input\n",
      "a, r, t\n",
      "a, r, t\n",
      "a, r, t\n",
      "a, r, t, x\n",
      "a, r, x\n",
      "Observation\n",
      "n/a\n",
      "n/a\n",
      "n/a\n",
      "1-hot\n",
      "RGB (84x84)\n",
      "No. trials/episode\n",
      "100\n",
      "5\n",
      "150\n",
      "10\n",
      "10\n",
      "Episode length\n",
      "100\n",
      "5\n",
      "150\n",
      "20\n",
      "≤3600\n",
      "Table 1: List of hyperparameters. βe = coefﬁcient of entropy regularization loss; in Exps. 1-4, βe is annealed\n",
      "from 1.0 to 0.0 over the course of training. βv = coefﬁcient of value function loss (Mirowski et al., 2016). r =\n",
      "reward, a = last action, t = current time step, x = current observation. Exp. 1: Bandits with independent arms\n",
      "(Section 3.1.1); Exp. 2: Bandits with dependent arms I (Section 3.1.2); Exp. 3: Bandits with dependent arms II\n",
      "(Section 3.1.3); Exp. 4: Restless bandits (Section 3.1.4); Exp. 5: The “Two-Step Task” (Section 3.2.1); Exp. 6:\n",
      "Learning abstract task structure (Section 3.2.2).\n",
      "action outputs are described in conjunction with individual experiments. In all experiments except\n",
      "where speciﬁed, the input included a scalar indicating the reward received on the preceding time-step\n",
      "as well as a one-hot representation of the action sampled on that time-step.\n",
      "All reinforcement learning was conducted using the Advantage Actor-Critic algorithm, as detailed\n",
      "in Mnih et al. (2016) and Mirowski et al. (2016) (see also Figure 1). Details of training, including\n",
      "the use of entropy regularization and a combined policy and value estimate loss, closely follow the\n",
      "methods detailed in Mirowski et al. (2016), with the exception that our experiments used a single\n",
      "thread unless otherwise noted. For a full listing of parameters refer to Table 1.\n",
      "Figure 1: Advantage actor-critic with recurrence. In all architectures, reward and last action are additional inputs\n",
      "to the LSTM. For non-bandit environments, observation is also fed into the LSTM either as a one-hot or passed\n",
      "through an encoder model [3-layer encoder: two convolutional layers (ﬁrst layer: 16 8x8 ﬁlters applied with\n",
      "stride 4, second layer: 32 4x4 ﬁlters with stride 2) followed by a fully connected layer with 256 units and then a\n",
      "ReLU non-linearity. See for details Mirowski et al. (2016)]. For bandit experiments, current time step is also\n",
      "fed in as input. π = policy; v = value function. A3C is the distributed multi-threaded asynchronous version\n",
      "of the advantage actor-critic algorithm (Mnih et al., 2016); A2C is single threaded. (a) Architecture used in\n",
      "experiments 1-5. (b) Convolutional-LSTM architecture used in experiment 6. (c) Stacked LSTM architecture\n",
      "with convolutional encoder used in experiments 6 and 7.\n",
      "4\n",
      "\f3.1\n",
      "BANDIT PROBLEMS\n",
      "As an initial setting for evaluating meta-RL, we studied a series of bandit problems. Except for a very\n",
      "limited set of bandit environments, it is intractable to compute the (prior-dependent) Bayesian-optimal\n",
      "strategy. Here we demonstrate that a recurrent system trained on a set of bandit environments drawn\n",
      "i.i.d. from a given distribution of environments produces a bandit algorithm which performs well on\n",
      "problems drawn from that distribution, and to a certain extent generalizes to related distributions.\n",
      "Thus, meta-RL learns a prior-dependent bandit algorithm.\n",
      "The speciﬁc bandit instantiation of the general meta-RL procedure described in Section 2.3 is deﬁned\n",
      "as follows. Let D be a training distribution over bandit environments. The meta-RL system is trained\n",
      "on a sequence of bandit environments through episodes. At the start of a new episode, its LSTM state\n",
      "is reset and a bandit task b ∼D is sampled. A bandit task is deﬁned as a set of distributions – one for\n",
      "each arm – from which rewards are sampled. The agent plays in this bandit environment for a certain\n",
      "number of trials and is trained to maximize observed rewards. After training, the agent’s policy is\n",
      "evaluated on a set of bandit tasks that are drawn from a test distribution D′, which can either be the\n",
      "same as D or a slight modiﬁcation of it.\n",
      "We evaluate the resulting performance of the learned bandit algorithm by the cumulative regret,\n",
      "a measure of the loss (in expected rewards) suffered when playing sub-optimal arms. Writing\n",
      "µa(b) the expected reward of arm a in bandit environment b, and µ∗(b) = maxa µa(b) = µa∗(b)(b)\n",
      "(where a∗(b) is one optimal arm) the optimal expected reward, we deﬁne the cumulative regret (in\n",
      "environment b) as RT (b) = PT\n",
      "t=1 µ∗(b) −µat(b), where at is the arm (action) chosen at time t. In\n",
      "experiment 4 (Restless bandits; Section 3.1.4), µ∗also depends on t. We report the performance\n",
      "(average over bandit environments drawn from the test distribution) either in terms of the cumulative\n",
      "regret: Eb∼D′[RT (b)] or in terms of number of sub-optimal pulls: Eb∼D′[PT\n",
      "t=1 I{at ̸= a∗(b)}].\n",
      "3.1.1\n",
      "BANDITS WITH INDEPENDENT ARMS\n",
      "We ﬁrst consider a simple two-armed bandit task to examine the behavior of meta-RL under conditions\n",
      "where theoretical guarantees exist and general purpose algorithms apply. The arm distributions are\n",
      "independent Bernoulli distributions (rewards are 1 with probability p and 0 with probability 1 −p),\n",
      "where the parameters of each arm (p1 and p2) are sampled independently and uniformly over [0, 1].\n",
      "We denote by Di the corresponding distribution over these independent bandit environments (where\n",
      "the subscript i stands for independent arms).\n",
      "At the beginning of each episode, a new bandit task is sampled and held constant for 100 trials.\n",
      "Training lasted for 20,000 episodes. The network is given as input the last reward, last action taken,\n",
      "and the trial number t, subsequently producing the action for the next trial t + 1 (Figure 1). After\n",
      "training, we evaluated on 300 new episodes with the learning rate set to zero (the learned policy is\n",
      "ﬁxed).\n",
      "Across model instances, we randomly sampled learning rate and discount, following Mnih et al. (2016).\n",
      "For all ﬁgures, we plotted the average of the top 5 runs of 100 randomly sampled hyperparameter\n",
      "settings, where the top agents were selected from the ﬁrst half of the 300 evaluation episodes and\n",
      "performance was plotted for the second half. We measured the cumulative expected regret across the\n",
      "episode, comparing with several algorithms tailored for this independent bandit setting: Gittins indices\n",
      "(Gittins, 1979) (which is Bayesian optimal in the ﬁnite-horizon case), UCB (Auer et al., 2002) (which\n",
      "comes with theoretical ﬁnite-time regret guarantees), and Thompson sampling (Thompson, 1933)\n",
      "(which is asymptotically optimal in this setting: see Kaufmann et al., 2012b). Model simulations\n",
      "were conducted with the PymaBandits toolbox from (Kaufmann et al., 2012a) and custom Matlab\n",
      "scripts.\n",
      "As shown in Figure 2a (green line; “Independent”), meta-RL outperforms both Thompson sampling\n",
      "(gray dashed line) and UCB (light gray dashed line), although it performs less well compared to\n",
      "Gittins (black dashed line). To verify the critical importance of providing reward information to the\n",
      "LSTM, we removed this input, leaving all other inputs as before. As expected, performance was at\n",
      "chance levels on all bandit tasks.\n",
      "5\n",
      "\fEpisodes\n",
      "300\n",
      "0\n",
      "Sub-optimal arm pulls\n",
      "0\n",
      "100\n",
      "(a)\n",
      "Training Condition\n",
      "Testing Condition\n",
      "Cumulative regret\n",
      "Indep.\n",
      "Unif.\n",
      "Easy\n",
      "Med.\n",
      "Hard\n",
      "Indep.\n",
      "Unif.\n",
      "Easy\n",
      "Med.\n",
      "Hard\n",
      "(b)\n",
      "(c)\n",
      "(d)\n",
      "(e)\n",
      "Trial #\n",
      "(f)\n",
      "Trial #\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "Testing: Dependent Uniform\n",
      "Cumulative Regret\n",
      "UCB\n",
      "LSTM A2C “Dependent Uniform”\n",
      "Thompson\n",
      "Gittins\n",
      "Trial #\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "Testing: Independent\n",
      "Cumulative Regret\n",
      "UCB\n",
      "LSTM A2C “Independent”\n",
      "Thompson\n",
      "Gittins\n",
      "Trial #\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "Testing: Hard\n",
      "Cumulative Regret\n",
      "4\n",
      "UCB\n",
      "LSTM A2C “Medium”\n",
      "Thompson\n",
      "Gittins\n",
      "Trial #\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "Testing: Easy\n",
      "Cumulative Regret\n",
      "UCB\n",
      "LSTM A2C “Medium”\n",
      "Thompson\n",
      "Gittins\n",
      "Figure 2: Performance on independent- and correlated-arm bandits. We report performance as the cumulative\n",
      "expected regret RT for 150 test episodes, averaged over the top 5 hyperparameters for each agent-task con-\n",
      "ﬁguration, where the top 5 was determined based on performance on a separate set of 150 test episodes. (a)\n",
      "LSTM A2C trained and evaluated on bandits with independent arms (distribution Di; see text), and compared\n",
      "with theoretically optimal models. (b) A single agent playing the medium difﬁculty task with distribution Dm.\n",
      "Suboptimal arm pulls over trials are depicted for 300 episodes. (c) LSTM A2C trained and evaluated on bandits\n",
      "with dependent uniform arms (distribution Du), (d) trained on medium bandit tasks (Dm) and tested on easy\n",
      "(De), and (e) trained on medium (Dm) and tested on hard task (Dh). (f) Cumulative regret for all possible\n",
      "combinations of training and testing environments (Di, Du, De, Dm, Dh).\n",
      "3.1.2\n",
      "BANDITS WITH DEPENDENT ARMS (I)\n",
      "As we have emphasized, a key property of meta-RL is that it gives rise to a learned RL algorithm that\n",
      "exploits consistent structure in the training distribution. In order to garner empirical evidence for this\n",
      "point, we tested the agent from our ﬁrst experiment in a more structured bandit task. Speciﬁcally,\n",
      "we trained the system on two-arm bandits in which arm reward distributions are correlated. In\n",
      "this setting, unlike the one studied in the previous section, experience with either arm provides\n",
      "information about the other. Standard bandit algorithms, including UCB and Thompson sampling,\n",
      "perform suboptimally in this setting, as they are not designed to exploit such correlations. In some\n",
      "cases it is possible to tailor algorithms for speciﬁc arm structures (see for example Lattimore and\n",
      "Munos, 2014), but extensive problem-speciﬁc analysis is typically required. Our approach aims to\n",
      "learn a structure-dependent bandit algorithm directly from experience with the target bandit domain.\n",
      "We consider Bernoulli distributions where the parameters (p1, p2) of the two arms are correlated\n",
      "in the sense that p1 = 1 −p2. We consider several training and test distributions. The uniform\n",
      "means that p1 ∼U([0, 1]) (uniform distribution over the unit interval). The easy means that\n",
      "p1 ∼U({0.1, 0.9}) (uniform distribution over those two possible values), and similarly we call\n",
      "medium when p1 ∼U({0.25, 0.75}) and hard when p1 ∼U({0.4, 0.6}). We denote by Du,\n",
      "De, Dm, and Dh the corresponding induced distributions over bandit environments. In addition\n",
      "6\n",
      "\fwe also considered the independent uniform distribution (as in the previous section, Di) where\n",
      "p1, p2 ∼U([0, 1]) independently. Agents were both trained and tested on those ﬁve distributions over\n",
      "bandit environments (among which four correspond to correlated distributions: Du, De, Dm and Dh;\n",
      "and one to the independent case: Di). As a validation of the names given to the task distributions\n",
      "(De, Dm, Dh), results show that the easy task is easier to learn than the medium which itself is easier\n",
      "than the hard one (Figure 2f). This is compatible with the general notion that the hardness of a bandit\n",
      "problem is inversely proportional to the difference between the expected reward of the optimal and\n",
      "sub-optimal arms. We again note that withholding the reward input to the LSTM resulted in chance\n",
      "performance on even the easiest bandit task, as should be expected.\n",
      "Figure 2f reports the results of all possible training-testing regimes. From observing the cumulative\n",
      "expected regrets, we make the following observations: i) agents trained in structured environments\n",
      "(Du, De, Dm, and Dh) develop prior knowledge that can be used effectively when tested on structured\n",
      "distributions – performing comparably to Gittins (Figure 2c-f), and superiorly compared to agents\n",
      "trained on independent arms (Di) in all structured tasks at test (Figure 2f). This is because an agent\n",
      "trained on independent rewards (Di) has not learned to exploit the reward correlations that are useful\n",
      "in those structured tasks. ii) Conversely, previous training on any structured distribution (Du, De,\n",
      "Dm, or Dh) hurts performance when agents are tested on an independent distribution (Di; Figure 2f).\n",
      "This makes sense, as training on correlated arms may produce a policy that relies on speciﬁc reward\n",
      "structure, thereby impacting performance in problems where no such structure exists. iii) Whilst\n",
      "the previous results emphasize the point that meta-RL gives rise to a separate learnt RL algorithm\n",
      "that implements prior-dependent bandit strategies, results also provide evidence that there is some\n",
      "generalization beyond the exact training distribution encountered (Figure 2f). For example, agents\n",
      "trained on the distributions De and Dm perform well when tested over a much wider structured\n",
      "distribution (i.e. Du). Further, our evidence suggests that there is generalization from training\n",
      "on the easier tasks (De,Dm) to testing on the hardest task (Dh; Figure 2e), with similar or even\n",
      "marginally superior performance as compared to training on the hard distribution Dh itself(Figure\n",
      "2f). In contrast, training on the hard distribution Dh results in relatively poor generalization to other\n",
      "structured distributions (Du, De, Dm), suggesting that training purely on hard instances may result in\n",
      "a learned RL algorithm that is more constrained by prior knowledge, perhaps due to the difﬁculty of\n",
      "solving the original problem.\n",
      "3.1.3\n",
      "BANDITS WITH DEPENDENT ARMS (II)\n",
      "In the previous experiment, the agent could outperform standard bandit algorithms by making use\n",
      "of learned dependencies between arms. However, it could do this while always choosing what it\n",
      "believes to be the highest-paying arm. We next examine a problem where information can be gained\n",
      "by paying a short-term reward cost. Similar problems have been examined before as providing a\n",
      "challenge to standard bandit algorithms (see e.g. Russo and Van Roy, 2014). In contrast, humans and\n",
      "animals make decisions that sacriﬁce immediate reward for information gain (e.g. Bromberg-Martin\n",
      "and Hikosaka, 2009).\n",
      "In this experiment, the agent was trained on 11-armed bandits with strong dependencies between\n",
      "arms. All arms had deterministic payouts. Nine “non-target” arms had reward = 1, and one “target”\n",
      "arm had reward = 5. Meanwhile, arm a11 was always “informative”, in that the target arm was\n",
      "indexed by 10 times a11’s reward (e.g. a reward of 0.2 on a11 indicated that a2 was the target arm).\n",
      "Thus, a11’s payouts ranged from 0.1 to 1. In each episode, the index of the target arm was randomly\n",
      "assigned. On the ﬁrst trial of each episode, the agent could not know which arm was the target, so the\n",
      "informative arm returned expected reward 0.55 and every target arm returned expected reward 1.4.\n",
      "Choosing the informative arm thus meant foregoing immediate reward, but with the compensation\n",
      "of valuable information. Episodes were ﬁve steps long. Again, the reward on the previous trial was\n",
      "provided as an additional observation to the agent. To facilitate learning, this was encoded in 1-hot\n",
      "format.\n",
      "Results are shown in Figure 3. The agent learned the optimal long-run strategy of sampling the\n",
      "informative arm once, despite the short-term cost, and then using the resulting information to exploit\n",
      "the high-value target arm. Thompson sampling, if supplied the true prior, searched potential target\n",
      "arms and exploited the target if found. UCB performed worse because it sampled every arm once\n",
      "even if the target arm was found early.\n",
      "7\n",
      "\fFigure 3: Learned RL procedure pays immediate cost to gain information to improve long-run returns. In this task,\n",
      "one arm is lower-paying but provides perfect information about which of the other ten arms is highest-paying.\n",
      "The remaining nine arms are intermediate in reward. The index of the informative arm is ﬁxed between episodes,\n",
      "but the index of the highest-paying arm is randomized between episodes. On the ﬁrst trial, the trained agent\n",
      "samples the informative arm. On subsequent trials, the agent uses the information it gained to deterministically\n",
      "exploit the highest-paying arm. Thompson sampling and UCB are not able to take advantage of the dependencies\n",
      "between arms.\n",
      "3.1.4\n",
      "RESTLESS BANDITS\n",
      "In previous experiments we considered stationary problems where the agent’s actions yielded in-\n",
      "formation about task parameters that remained ﬁxed throughout each episode. Next, we consider a\n",
      "bandit problem in which reward probabilities change over the course of an episode, with different\n",
      "rates of change (volatilities) in different episodes. To perform well, the agent must not only track\n",
      "the best arm, but also infer the volatility of the episode and adjust its own learning rate accordingly.\n",
      "In such an environment, learning rates should be higher when the environment is changing rapidly,\n",
      "because past information becomes irrelevant more quickly (Behrens et al., 2007; Sutton and Barto,\n",
      "1998).\n",
      "We tested whether meta-RL would learn such a ﬂexible RL policy using a two-armed Bernoulli bandit\n",
      "task with reward probabilities p1 and 1-p1. The value of p1 changed slowly in “low vol” episodes\n",
      "and quickly in “high vol” episodes. The agent had no way of knowing which type of episode it\n",
      "was in, except for its reward history within the episode. Figure 4a shows example “low vol” and\n",
      "“high vol” episodes. Reward magnitude was ﬁxed at 1, and episodes were 100 steps long. UCB and\n",
      "Thompson sampling were again implemented for comparison. The conﬁdence bound term\n",
      "q\n",
      "χ log n\n",
      "ni\n",
      "in UCB had parameter χ which was set to 1, selected empirically for good performance on our data\n",
      "set. Thompson sampling’s posterior update included knowledge of the Gaussian random walk, but\n",
      "with a ﬁxed volatility for all episodes.\n",
      "As in the previous experiment, meta-RL achieved lower regret in test than Thompson sampling,\n",
      "UCB, or the Rescorla-Wagner (R-W) learning rule (Figure 4b; Rescorla et al., 1972) with the best\n",
      "ﬁxed learning rate (α=0.5). To test whether the agent adjusted its effective learning rate to match\n",
      "environments with different volatility levels, we ﬁt R-W models to the agent’s behavior, concatenating\n",
      "episodes into blocks of 10, where each block consisted of only “low vol” or only “high vol” episodes.\n",
      "We considered four different models encompassing different combinations of three parameters:\n",
      "learning rate α, softmax inverse temperature β, and a lapse rate ϵ to account for unexplained choice\n",
      "variance not related to estimated value Economides et al. (2015). Model “b” included only β, “ab”\n",
      "included α and β, “be” included β and ϵ, and “abe” included all three. All parameters were estimated\n",
      "separately on each block of 10 episodes. In models where ϵ and α were not free, they were ﬁxed\n",
      "to 0 and 0.5, respectively. Model comparison by Bayesian Information Criterion (BIC) indicated\n",
      "that meta-RL’s behavior was better described by a model with different learning rates for each block\n",
      "than a model with a ﬁxed learning rate across blocks. As a control, we performed the same model\n",
      "comparison on the behavior produced by the best R-W agent, ﬁnding no beneﬁt of allowing different\n",
      "learning rates across episodes (models “abe” and “ab” vs “be” and “b”; Figure 4c-d). In these models,\n",
      "the parameter estimates for meta-RL’s behavior were strongly related to the volatility of the episodes,\n",
      "indicating that meta-RL adjusted its learning rate to the volatility of the episode, whereas model\n",
      "ﬁtting the R-W behavior simply recovered the ﬁxed parameters (Figure 4e-f).\n",
      "8\n",
      "\fFigure 4: Learned RL procedure adapts its own learning rate to the environment. (a) Agents were trained on\n",
      "two-armed bandits with perfectly anti-correlated Bernoulli reward probabilities, p1 and 1-p1. Two example\n",
      "episodes are shown. p1 changed within an episode (solid black line), with a fast Poisson jump rate in “high vol”\n",
      "episodes and a slow rate in “low vol” episodes. (b) The trained LSTM agent outperformed UCB, Thompson\n",
      "sampling, and a Rescorla-Wagner (R-W) learner with ﬁxed learning rate α=0.5 (selected for being optimal on\n",
      "average in this distribution of environments). (c,d) We ﬁt R-W models by maximum likelihood both to the\n",
      "behavior of R-W (as a control) and to the behavior of LSTM. Models including a learning rate that could vary\n",
      "between episodes (“ab” and “abe”) outperformed models without these free parameters on LSTM’s data, but not\n",
      "on R-W’s data. Addition of a lapse parameter further improved model ﬁts on LSTM’s data (“be” and “abe”),\n",
      "suggesting that the algorithm implemented by LSTM is not exactly Rescorla-Wagner. (e,f) The LSTM’s, but not\n",
      "R-W’s, estimated learning rate was higher in volatile episodes. Small jitter added to visualize overlapping points.\n",
      "3.2\n",
      "MARKOV DECISION PROBLEMS\n",
      "The foregoing experiments focused on bandit tasks in which actions do not affect the task’s underlying\n",
      "state. We turn now to MDPs where actions do inﬂuence state. We begin with a task derived from the\n",
      "neuroscience literature and then turn to a task, originally studied in the context of animal learning,\n",
      "which requires learning of abstract task structure. As in the previous experiments, our focus is\n",
      "on examining how meta-RL adapts to invariances in task structure. We wrap up by reviewing an\n",
      "experiment recently reported in a related paper (Mirowski et al., 2016), which demonstrates how\n",
      "meta-RL can scale to large-scale navigation tasks with rich visual inputs.\n",
      "3.2.1\n",
      "THE “TWO-STEP TASK”\n",
      "Here we examine meta-RL in a setting that has been widely used in the neuroscience literature to\n",
      "distinguish the contribution of different systems viewed to support decision making (Daw et al.,\n",
      "2005). Speciﬁcally, this paradigm – known as the “two-step task” (Daw et al., 2011) – was developed\n",
      "to dissociate a model-free system that caches values of actions in states (e.g. TD(1) Q-learning;\n",
      "see Sutton and Barto, 1998), from a model-based system which learns an internal model of the\n",
      "environment and evaluates the value of actions at the time of decision-making through look-ahead\n",
      "planning (Daw et al., 2005). Our interest was in whether meta-RL would give rise to behavior\n",
      "emulating a model-based strategy, despite the use of a model-free algorithm (in this case A2C) to\n",
      "train the system weights.\n",
      "9\n",
      "\fWe used a modiﬁed version of the two-step task, designed to bolster the utility of model-based over\n",
      "model-free control (see Kool et al., 2016). The task’s structure is diagrammed in Figure 5a. From the\n",
      "ﬁrst-stage state S1, action a1 leads to second-stage states S2 and S3 with probability 0.75 and 0.25,\n",
      "respectively, while action a2 leads to S2 and S3 with probabilities 0.25 and 0.75. One second-stage\n",
      "state yielded a reward of 1.0 with probability 0.9 (and otherwise zero); the other yielded the same\n",
      "reward with probability 0.1. The identity of the higher-valued state was assigned randomly for each\n",
      "episode. Thus, the expected values for the two ﬁrst-stage actions were either ra = 0.9 and rb = 0.1, or\n",
      "ra = 0.1 and rb = 0.9. All three states were represented by one-hot vectors, with the transition model\n",
      "held constant across episodes: i.e. only the expected value of the second stage states changed from\n",
      "episode to episode.\n",
      "We applied the conventional analysis used in the neuroscience literature to dissociate model-free\n",
      "from model-based control (Daw et al., 2011). This focuses on the “stay probability,” that is, the\n",
      "probability with which a ﬁrst-stage action is selected at trial t + 1 following a second-stage reward\n",
      "at trial t, as a function of whether trial t involved a common transition (e.g. action a1 at state S1\n",
      "led to S2) or rare transition (action a2 at state S1 led to S3). Under the standard interpretation (see\n",
      "Daw et al., 2011), model-free control – à la TD(1) – predicts that there should be a main effect of\n",
      "reward: First-stage actions will tend to be repeated if followed by reward, regardless of transition\n",
      "type, and such actions will tend not to be repeated (choice switch) if followed by non-reward (Figure\n",
      "5b). In contrast, model-based control predicts an interaction between the reward and transition type,\n",
      "reﬂecting a more goal-directed strategy, which takes the transition structure into account. Intuitively,\n",
      "if you receive a second-stage reward (e.g. at S2) following a rare transition (i.e. having taken action\n",
      "a2 at state S1), to maximize your chances of getting to this reward on the next trial based on your\n",
      "knowledge of the transition structure, the optimal ﬁrst stage action is a1 (i.e. switch).\n",
      "The results of the stay-probability analysis performed on the agent’s choices show a pattern conven-\n",
      "tionally interpreted as implying the operation of model-based control (Figure 5c). As in previous\n",
      "experiments, when reward information was withheld at the level of network input, performance was\n",
      "at chance levels.\n",
      "If interpreted following standard practice in neuroscience, the behavior of the model in this experiment\n",
      "reﬂects a surprising effect: training with model-free RL gives rise to behavior reﬂecting model-based\n",
      "control. We hasten to note that different interpretations of the observed pattern of behavior are\n",
      "available (Akam et al., 2015), a point to which we will return below. However, notwithstanding this\n",
      "caveat, the results of the present experiment provide a further illustration of the point that the learning\n",
      "procedure that emerges from meta-RL can differ starkly from the original RL algorithm used to train\n",
      "the network weights, and takes a form that exploits consistent task structure.\n",
      "3.2.2\n",
      "LEARNING ABSTRACT TASK STRUCTURE\n",
      "In the ﬁnal experiment we conducted, we took a step towards examining the scalabilty of meta-RL, by\n",
      "studying a task that involves rich visual inputs, longer time horizons and sparse rewards. Additionally,\n",
      "in this experiment we studied a meta-learning task that requires the system to tune into an abstract\n",
      "task structure, in which a series of objects play deﬁned roles which the system must infer.\n",
      "The task was adapted from a classic study of animal behavior, conducted by Harlow (1949). On each\n",
      "trial in the original task, Harlow presented a monkey with two visually contrasting objects. One of\n",
      "these covered a small well containing a morsel of food; the other covered an empty well. The animal\n",
      "chose freely between the two objects and could retrieve the food reward if present. The stage was\n",
      "then hidden and the left-right positions of the objects were randomly reset. A new trial then began,\n",
      "with the animal again choosing freely. This process continued for a set number of trials using the\n",
      "same two objects. At completion of this set of trials, two entirely new and unfamiliar objects were\n",
      "substituted for the original two, and the process began again. Importantly, within each block of trials,\n",
      "one object was chosen to be consistently rewarded (regardless of its left-right position), with the other\n",
      "being consistently unrewarded. What Harlow (Harlow, 1949) observed was that, after substantial\n",
      "practice, monkeys displayed behavior that reﬂected an understanding of the task’s rules. When two\n",
      "new objects were presented, the monkey’s ﬁrst choice between them was necessarily arbitrary. But\n",
      "after observing the outcome of this ﬁrst choice, the monkey was at ceiling thereafter, always choosing\n",
      "the rewarded object.\n",
      "10\n",
      "\f(a) Two-step task\n",
      "(b) Model predictions\n",
      "(c) LSTM A2C with reward input\n",
      "Figure 5: Three-state MDP modeled after the “two-step task” from Daw et al. (2011). (a) MDP with 3 states and\n",
      "2 actions. All trials start in state S1, with transition probabilities after taking actions a1 or a2 depicted in the\n",
      "graph. S2 and S3 result in expected rewards ra and rb (see text). (b) Predictions of choice probabilities given\n",
      "either a model-based strategy or a model-free strategy (Daw et al., 2011). Speciﬁcally, model-based strategies\n",
      "take into account transition probabilities and would predict an interaction between the amount of reward received\n",
      "on the last trial and the transition (common or uncommon) observed. (c) Agent displays a perfectly model-based\n",
      "proﬁle when given the reward as input.\n",
      "We anticipated that meta-RL should give rise to the same pattern of abstract one-shot learning. In\n",
      "order to test this, we adapted Harlow’s paradigm into a visual ﬁxation task, as follows. A 84x84 pixel\n",
      "input represented a simulated computer screen (see Figure 6a-c). At the beginning of each trial, this\n",
      "display was blank except for a small central ﬁxation cross (red crosshairs). The agent selected discrete\n",
      "left-right actions which shifted its view approximately 4.4 degrees in the corresponding direction,\n",
      "with a small momentum effect (alternatively, a no-op action could be selected). The completion of a\n",
      "trial required performing two tasks: saccading to the central ﬁxation cross, followed by saccading\n",
      "to the correct image. If the agent held the ﬁxation cross in the center of the ﬁeld of view (within a\n",
      "tolerance of 3.5 degrees visual angle) for a minimum of four time steps, it received a reward of 0.2.\n",
      "The ﬁxation cross then disappeared and two images – drawn randomly from the ImageNet dataset\n",
      "(Deng et al., 2009) and resized to 34x34 – appeared on the left and right side of the display (Figure\n",
      "6b). The agent’s task was then to “select” one of the images by rotating until the center of the image\n",
      "aligned with the center of the visual ﬁeld of view (within a tolerance of 7 degrees visual angle).\n",
      "Once one of the images was selected, both images disappeared and, after an intertrial interval of 10\n",
      "time-steps, the ﬁxation cross reappeared, initiating the next trial. Each episode contained a maximum\n",
      "of 10 trials or 3600 steps. Following Mirowski et al. (2016), we implemented an action repeat of 4,\n",
      "meaning that selecting an image took a minimum of three independent decisions (twelve primitive\n",
      "actions) after having completed the ﬁxation. It should be noted, however, that the rotational position\n",
      "of the agent was not limited; that is, 360 degree rotations could occur, while the simulated computer\n",
      "screen only subtended 65 degrees.\n",
      "Although new ImageNet images were chosen at the beginning of each episode (sampled with\n",
      "replacement from a set of 1000 images), the same images were re-used across all trials within\n",
      "an episode, though in randomly varying left-right placement, similar to the objects in Harlow’s\n",
      "experiment. And as in that experiment, one image was arbitrarily chosen to be the “rewarded” image\n",
      "throughout the episode. Selection of this image yielded a reward of 1.0, while the other image yielded\n",
      "a reward of -1.0. During test, the A3C learning rate was set to zero and ImageNet images were drawn\n",
      "from a separate held-out set of 1000, never presented during training.\n",
      "A grid search was conducted for optimal hyperparameters. At perfect performance, agents can\n",
      "complete one trial per 20-30 steps and achieve a maximum expected reward of 9 per 10 trials. Given\n",
      "11\n",
      "\f(a) Fixation\n",
      "(b) Image display\n",
      "(c) Right saccade and selection\n",
      "(d) Training performance\n",
      "(e) Robustness over random seeds\n",
      "(f) One-shot learning\n",
      "Figure 6: Learning abstract task structure in visually rich 3D environment. a-c) Example of a single trial,\n",
      "beginning with a central ﬁxation, followed by two images with random left-right placement. d) Average\n",
      "performance (measured in average reward per trial) of top 40 out of 100 seeds during training. Maximum\n",
      "expected performance is indicated with black dashed line. e) Performance at episode 100,000 for 100 random\n",
      "seeds, in decreasing order of performance. f) Probability of selecting the rewarded image, as a function of trial\n",
      "number for a single A3C stacked LSTM agent for a range of training durations (episodes per thread, 32 threads).\n",
      "the nature of the task – which requires one-shot image-reward memory together with maintenance of\n",
      "this information over a relatively long timescale (i.e. over ﬁxation-cross selections and across trials) –\n",
      "we assessed the performance of not only a convolutional-LSTM architecture which receives reward\n",
      "and action as additional input (see Figure 1b and Table 1), but also a convolutional-stacked LSTM\n",
      "architecture used in a navigation task discussed below (see Figure 1c).\n",
      "Agent performance is illustrated in Figure 6d-f. Whilst the single LSTM agent was relatively\n",
      "successful at solving the task, the stacked-LSTM variant exhibited much better robustness. That is,\n",
      "43% of random seeds of the best hyperparameter set performed at ceiling (Figure 6e), compared to\n",
      "26% of the single LSTM.\n",
      "Like the monkeys in Harlow’s experiment (Harlow, 1949), the networks converge on an optimal\n",
      "policy: Not only does the agent successfully ﬁxate to begin each trial, but starting on the second trial\n",
      "of each episode it invariably selects the rewarded image, regardless of which image it selected on the\n",
      "ﬁrst trial(Figure 6f). This reﬂects an impressive form of one-shot learning, which reﬂects an implicit\n",
      "understanding of the task structure: After observing one trial outcome, the agent binds a complex,\n",
      "unfamiliar image to a speciﬁc task role.\n",
      "Further experiments, reported elsewhere (Wang et al., 2017), conﬁrmed that the same recurrent\n",
      "A3C system is also able to solve a substantially more difﬁcult version of the task. In this task, only\n",
      "one image – which was randomly designated to be either the rewarding item to be selected, or the\n",
      "unrewarding item to be avoided – was presented on every trial during an episode, with the other\n",
      "image presented being novel on every trial.\n",
      "3.2.3\n",
      "ONE-SHOT NAVIGATION\n",
      "The experiments using the Harlow task demonstrate the capacity of meta-RL to operate effectively\n",
      "within a visually rich environment, with relatively long time horizons. Here we consider related\n",
      "experiments recently reported within the navigation domain (Mirowski et al., 2016) (see also Jaderberg\n",
      "et al., 2016), and discuss how these can be recast as examples of meta-RL – attesting to the scaleability\n",
      "of this principle to more typical MDP settings that pose challenging RL problems due to dynamically\n",
      "changing sparse rewards.\n",
      "12\n",
      "\f(a) Labryinth I-maze\n",
      "(b) Illustrative Episode\n",
      "(c) Performance\n",
      "(d) Value Function\n",
      "Figure 7: a) view of I-maze showing goal object in one of the 4 alcoves b) following initial exploration\n",
      "(light trajectories), agent repeatedly goes to goal (blue trajectories) c) Performance of stacked LSTM (termed\n",
      "“Nav A3C”) and feedforward (“FF A3C”) architectures, per episode (goal = 10 points) averaged across top 5\n",
      "hyperparameters. e) following initial goal discovery (goal hits marked in red), value function occurs well in\n",
      "advance of the agent seeing the goal which is hidden in an alcove. Figure used with permission from Mirowski\n",
      "et al. (2016).\n",
      "Speciﬁcally, we consider a setting where the environment layout is ﬁxed but the goal changes location\n",
      "randomly each episode (Figure 7; Mirowski et al., 2016). Although the layout is relatively simple,\n",
      "the Labyrinth environment (see for details Mirowski et al., 2016) is richer and more ﬁnely discretized\n",
      "(cf VizDoom), resulting in long time horizons; a trained agent takes approximately 100 steps (10\n",
      "seconds) to reach the goal for the ﬁrst time in a given episode. Results show that a stacked LSTM\n",
      "architecture (Figure 1c), that receives reward and action as additional inputs equivalent to that used\n",
      "in our Harlow experiment achieves near-optimal behavior – showing one-shot memory for the goal\n",
      "location after an initial exploratory period, followed by repeated exploitation (see Figure 7c). This is\n",
      "evidenced by a substantial decrease in latency to reach the goal for the ﬁrst time (~100 timesteps)\n",
      "compared to subsequent visits (~30 timesteps). Notably, a feedforward network (see Figure 7c),\n",
      "that receives only a single image as observation, is unable to solve the task (i.e. no decrease in\n",
      "latency between successive goal rewards). Whilst not interpreted as such in Mirowski et al. (2016),\n",
      "this provides a clear demonstration of the effectiveness of meta-RL: a separate RL algorithm with\n",
      "the capability of one-shot learning emerges through training with a ﬁxed and more incremental RL\n",
      "algorithm (i.e. policy gradient). Meta-RL can be viewed as allowing the agent to infer the optimal\n",
      "value function following initial exploration (see Figure 7d) – with the additional LSTM providing\n",
      "information about the currently relevant goal location to the LSTM that outputs the policy over the\n",
      "extended timeframe of the episode. Taken together, meta-RL allows a base model-free RL algorithm\n",
      "to solve a challenging RL problem that might otherwise require fundamentally different approaches\n",
      "(e.g. based on successor representations or fully model-based RL).\n",
      "4\n",
      "RELATED WORK\n",
      "We have already touched on the relationship between deep meta-RL and pioneering work by Hochre-\n",
      "iter et al. (2001) using recurrent networks to perform meta-learning in the setting of full supervision\n",
      "13\n",
      "\f(see also Cotter and Conwell, 1990; Prokhorov et al., 2002; Younger et al., 1999). That approach was\n",
      "recently extended in Santoro et al. (2016), which demonstrated the utility of leveraging an external\n",
      "memory structure. The idea of crossing meta-learning with reinforcement learning has been previ-\n",
      "ously discussed by Schmidhuber et al. (1996). That work, which appears to have introduced the term\n",
      "“meta-RL,” differs from ours in that it did not involve a neural network implementation. More recently,\n",
      "however, there has been a surge of interest in using neural networks to learn optimization procedures,\n",
      "using a range of innovative meta-learning techniques (Andrychowicz et al., 2016; Chen et al., 2016;\n",
      "Li and Malik, 2016; Zoph and Le, 2016). Recent work by Chen et al. (2016) is particularly close in\n",
      "spirit to the work we have presented here, and can be viewed as treating the case of “inﬁnite bandits”\n",
      "using a meta-learning strategy broadly analogous to the one we have pursued.\n",
      "The present research also bears a close relationship with a different body of recent work that has not\n",
      "been framed in terms of meta-learning. A number of studies have used deep RL to train recurrent\n",
      "neural networks on navigation tasks, where the structure of the task (e.g., goal location or maze\n",
      "conﬁguration) varies across episodes (Jaderberg et al., 2016; Mirowski et al., 2016). The ﬁnal\n",
      "experiment that we presented above, drawn from (Mirowski et al., 2016), is one example. To the\n",
      "extent that such experiments involve the key ingredients of deep meta-RL – a neural network with\n",
      "memory, trained through RL on a series of interrelated tasks – they are almost certain to involve the\n",
      "kind of meta-learning we have described in the present work. This related work provides an indication\n",
      "that meta-RL can be fruitfully applied to larger scale problems than the ones we have studied in our\n",
      "own experiments. Importantly, it indicates that a key ingredient in scaling the approach may be to\n",
      "incorporate memory mechanisms beyond those inherent in unstructured recurrent neural networks\n",
      "(see Graves et al., 2016; Mirowski et al., 2016; Santoro et al., 2016; Weston et al., 2014). Our work,\n",
      "for its part, suggests that there is untapped potential in deep recurrent RL agents to meta-learn quite\n",
      "abstract aspects of task structure, and to discover strategies that exploit such structure toward rapid,\n",
      "ﬂexible adaptation.\n",
      "During completion of the present research, closely related work was reported by Duan et al. (2016).\n",
      "Like us, Duan and colleagues use deep RL to train a recurrent network on a series of interrelated tasks,\n",
      "with the result that the network dynamics learn a second RL procedure which operates on a faster\n",
      "time-scale than the original algorithm. They compare the performance of these learned procedures\n",
      "against conventional RL algorithms in a number of domains, including bandits and navigation.\n",
      "An important difference between this parallel work and our own is the former’s primary focus on\n",
      "relatively unstructured task distributions (e.g., uniformly distributed bandit problems and random\n",
      "MDPs); our main interest, in contrast, has been in structured task distributions (e.g., dependent\n",
      "bandits and the task introduced by Harlow, 1949), because it is in this setting where the system can\n",
      "learn a biased – and therefore efﬁcient – RL procedure that exploits regular task structure. The two\n",
      "perspectives are, in this regard, nicely complementary.\n",
      "5\n",
      "CONCLUSION\n",
      "A current challenge in artiﬁcial intelligence is to design agents that can adapt rapidly to new tasks by\n",
      "leveraging knowledge acquired through previous experience with related activities. In the present\n",
      "work we have reported initial explorations of what we believe is one promising avenue toward this\n",
      "goal. Deep meta-RL involves a combination of three ingredients: (1) Use of a deep RL algorithm\n",
      "to train a recurrent neural network, (2) a training set that includes a series of interrelated tasks, (3)\n",
      "network input that includes the action selected and reward received in the previous time interval.\n",
      "The key result, which emerges naturally from the setup rather than being specially engineered, is\n",
      "that the recurrent network dynamics learn to implement a second RL procedure, independent from\n",
      "and potentially very different from the algorithm used to train the network weights. Critically, this\n",
      "learned RL algorithm is tuned to the shared structure of the training tasks. In this sense, the learned\n",
      "algorithm builds in domain-appropriate biases, which can allow it to operate with greater efﬁciency\n",
      "than a general-purpose algorithm. This bias effect was particularly evident in the results of our\n",
      "experiments involving dependent bandits (sections 3.1.2 and 3.1.3), where the system learned to\n",
      "take advantage of the task’s covariance structure; and in our study of Harlow’s animal learning task\n",
      "(section 3.2.2), where the recurrent network learned to exploit the task’s structure in order to display\n",
      "one-shot learning with complex novel stimuli.\n",
      "14\n",
      "\fOne of our experiments (section 3.2.1) illustrated the point that a system trained using a model-free\n",
      "RL algorithm can develop behavior that emulates model-based control. A few further comments on\n",
      "this result are warranted. As noted in our presentation of the simulation results, the pattern of choice\n",
      "behavior displayed by the network has been considered in the cognitive and neuroscience literatures\n",
      "as reﬂecting model-based control or tree search. However, as has been remarked in very recent work,\n",
      "the same pattern can arise from a model-free system with an appropriate state representation (Akam\n",
      "et al., 2015). Indeed, we suspect this may be how our network in fact operates. However, other\n",
      "ﬁndings suggest that a more explicitly model-based control mechanism can emerge when a similar\n",
      "system is trained on a more diverse set of tasks. In particular, Ilin et al. (2007) showed that recurrent\n",
      "networks trained on random mazes can approximate dynamic programming procedures (see also\n",
      "Silver et al., 2017; Tamar et al., 2016). At the same time, as we have stressed, we consider it an\n",
      "important aspect of deep meta-RL that it yields a learned RL algorithm that capitalizes on invariances\n",
      "in task structure. As a result, when faced with widely varying but still structured environments, deep\n",
      "meta-RL seems likely to generate RL procedures that occupy a grey area between model-free and\n",
      "model-based RL.\n",
      "The two-step decision problem studied in Section 3.2.1 was derived from neuroscience, and we\n",
      "believe deep meta-RL may have important implications in that arena (Wang et al., 2017). The notion\n",
      "of meta-RL has been discussed previously in neuroscience but only in a narrow sense, according\n",
      "to which meta-learning adjusts scalar hyperparameters such as the learning rate or softmax inverse\n",
      "temperature (Khamassi et al., 2011; 2013; Kobayashi et al., 2009; Lee and Wang, 2009; Schweighofer\n",
      "and Doya, 2003; Soltani et al., 2006). In recent work (Wang et al., 2017) we have shown that\n",
      "deep meta-RL can account for a wider range of experimental observations, providing an integrative\n",
      "framework for understanding the respective roles of dopamine and the prefrontal cortex in biological\n",
      "reinforcement learning.\n",
      "ACKNOWLEDGEMENTS\n",
      "We would like the thank the following colleagues for useful discussion and feedback: Nando de\n",
      "Freitas, David Silver, Koray Kavukcuoglu, Daan Wierstra, Demis Hassabis, Matt Hoffman, Piotr\n",
      "Mirowski, Andrea Banino, Sam Ritter, Neil Rabinowitz, Peter Dayan, Peter Battaglia, Alex Lerchner,\n",
      "Tim Lillicrap and Greg Wayne.\n",
      "REFERENCES\n",
      "Thomas Akam, Rui Costa, and Peter Dayan. Simple plans or sophisticated habits? state, transition and learning\n",
      "interactions in the two-step task. PLoS Comput Biol, 11(12):e1004648, 2015.\n",
      "Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando\n",
      "de Freitas. Learning to learn by gradient descent by gradient descent. arXiv preprint arXiv:1606.04474, 2016.\n",
      "Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.\n",
      "Machine learning, 47(2-3):235–256, 2002.\n",
      "Timothy EJ Behrens, Mark W Woolrich, Mark E Walton, and Matthew FS Rushworth. Learning the value of\n",
      "information in an uncertain world. Nature neuroscience, 10(9):1214–1221, 2007.\n",
      "Ethan S Bromberg-Martin and Okihide Hikosaka. Midbrain dopamine neurons signal preference for advance\n",
      "information about upcoming rewards. Neuron, 63(1):119–126, 2009.\n",
      "Yutian Chen, Matthew W Hoffman, Sergio Gomez, Misha Denil, Timothy P Lillicrap, and Nando de Freitas.\n",
      "Learning to learn for global optimization of black box functions. arXiv preprint arXiv:1611.03824, 2016.\n",
      "NE Cotter and PR Conwell. Fixed-weight networks can learn. In 1990 IJCNN International Joint Conference on\n",
      "Neural Networks, pages 553–559, 1990.\n",
      "Nathaniel D Daw, Yael Niv, and Peter Dayan. Uncertainty-based competition between prefrontal and dorsolateral\n",
      "striatal systems for behavioral control. Nature neuroscience, 8(12):1704–1711, 2005.\n",
      "Nathaniel D Daw, Samuel J Gershman, Ben Seymour, Peter Dayan, and Raymond J Dolan. Model-based\n",
      "inﬂuences on humans’ choices and striatal prediction errors. Neuron, 69(6):1204–1215, 2011.\n",
      "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\n",
      "image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages\n",
      "248–255. IEEE, 2009.\n",
      "Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement\n",
      "learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016. URL http://arxiv.\n",
      "15\n",
      "\forg/abs/1611.02779.\n",
      "Marcos Economides, Zeb Kurth-Nelson, Annika Lübbert, Marc Guitart-Masip, and Raymond Dolan. Model-\n",
      "based reasoning in humans becomes automatic with training. PLoS Computational Biology, 11(9):e1004463,\n",
      "2015.\n",
      "John C Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical Society. Series\n",
      "B (Methodological), pages 148–177, 1979.\n",
      "Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwi´nska,\n",
      "Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing\n",
      "using a neural network with dynamic external memory. Nature, 2016.\n",
      "Harry F Harlow. The formation of learning sets. Psychological review, 56(1):51, 1949.\n",
      "Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,\n",
      "1997.\n",
      "Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In\n",
      "International Conference on Artiﬁcial Neural Networks, pages 87–94. Springer, 2001.\n",
      "Roman Ilin, Robert Kozma, and Paul J Werbos. Efﬁcient learning in cellular simultaneous recurrent neural\n",
      "networks-the case of maze navigation problem. In 2007 IEEE International Symposium on Approximate\n",
      "Dynamic Programming and Reinforcement Learning, pages 324–329. IEEE, 2007.\n",
      "Max Jaderberg, Volodymir Mnih, Wojciech Czarnecki, Tom Schaul, Joel Z. Leibo, David Silver, and Koray\n",
      "Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397,\n",
      "2016. URL http://arxiv.org/abs/1611.05397.\n",
      "Emilie Kaufmann, Olivier Cappé, and Aurélien Garivier. On bayesian upper conﬁdence bounds for bandit\n",
      "problems. In Proc. of Int’l Conf. on Artiﬁcial Intelligence and Statistics, AISTATS, 2012a.\n",
      "Emilie Kaufmann, Nathaniel Korda, and Rémi Munos. Thompson sampling: An asymptotically optimal\n",
      "ﬁnite-time analysis. In Algorithmic Learning Theory - 23rd International Conference, pages 199–213, 2012b.\n",
      "Mehdi Khamassi, Stéphane Lallée, Pierre Enel, Emmanuel Procyk, and Peter F Dominey. Robot cognitive\n",
      "control with a neurophysiologically inspired reinforcement learning model. Frontiers in neurorobotics, 5:1,\n",
      "2011.\n",
      "Mehdi Khamassi, Pierre Enel, Peter Ford Dominey, and Emmanuel Procyk. Medial prefrontal cortex and the\n",
      "adaptive regulation of reinforcement learning parameters. Prog Brain Res, 202:441–464, 2013.\n",
      "Kunikazu Kobayashi, Hiroyuki Mizoue, Takashi Kuremoto, and Masanao Obayashi. A meta-learning method\n",
      "based on temporal difference error. In International Conference on Neural Information Processing, pages\n",
      "530–537. Springer, 2009.\n",
      "Wouter Kool, Fiery A Cushman, and Samuel J Gershman. When does model-based control pay off? PLoS\n",
      "Comput Biol, 12(8):e1005090, 2016.\n",
      "Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that\n",
      "learn and think like people. arXiv preprint arXiv:1604.00289, 2016.\n",
      "Tor Lattimore and Rémi Munos. Bounded regret for ﬁnite-armed structured bandits. In Advances in Neural\n",
      "Information Processing Systems 27, pages 550–558, 2014.\n",
      "Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444, 2015.\n",
      "Daeyeol Lee and Xiao-Jing Wang. Mechanisms for stochastic decision making in the primate frontal cortex:\n",
      "Single-neuron recording and circuit modeling. Neuroeconomics: Decision making and the brain, pages\n",
      "481–501, 2009.\n",
      "Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.\n",
      "Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andy Ballard, Andrea Banino, Misha Denil, Ross\n",
      "Goroshin, Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, and Raia Hadsell. Learning to navigate in\n",
      "complex environments. arXiv preprint arXiv:1611.03673, 2016. URL http://arxiv.org/abs/1611.\n",
      "03673.\n",
      "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, et al. Human-level control\n",
      "through deep reinforcement learning. Nature, 518:529–533, 2015.\n",
      "Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley,\n",
      "David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proc. of\n",
      "Int’l Conf. on Machine Learning, ICML, 2016.\n",
      "16\n",
      "\fDanil V Prokhorov, Lee A Feldkamp, and Ivan Yu Tyukin. Adaptive behavior with ﬁxed weights in rnn: an\n",
      "overview. In Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN), pages\n",
      "2018–2023, 2002.\n",
      "Robert A Rescorla, Allan R Wagner, et al. A theory of pavlovian conditioning: Variations in the effectiveness of\n",
      "reinforcement and nonreinforcement. Classical conditioning II: Current research and theory, 2:64–99, 1972.\n",
      "Dan Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. In Advances in\n",
      "Neural Information Processing Systems 27, pages 1583–1591, 2014.\n",
      "Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning\n",
      "with memory-augmented neural networks. In Proceedings of The 33rd International Conference on Machine\n",
      "Learning, pages 1842–1850, 2016.\n",
      "Jurgen Schmidhuber, Jieyu Zhao, and Marco Wiering. Simple principles of metalearning. Technical report, SEE,\n",
      "1996.\n",
      "Nicolas Schweighofer and Kenji Doya. Meta-learning in reinforcement learning. Neural Networks, 16(1):5–9,\n",
      "2003.\n",
      "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian\n",
      "Schrittwieser, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):\n",
      "484–489, 2016.\n",
      "David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold,\n",
      "David Reichert, Neil Rabinowitz, Andre Barreto, and Thomas Degris. The predictron: End-to-end learning\n",
      "and planning. Submitted to Int’l Conference on Learning Representations, ICLR, 2017.\n",
      "Alireza Soltani, Daeyeol Lee, and Xiao-Jing Wang. Neural mechanism for stochastic behaviour during a\n",
      "competitive game. Neural Networks, 19(8):1075–1090, 2006.\n",
      "Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press\n",
      "Cambridge, 1998.\n",
      "Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks. arXiv preprint\n",
      "arXiv:1602.02867v2, 2016.\n",
      "William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence\n",
      "of two samples. Biometrika, 25:285–294, 1933.\n",
      "Sebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning to learn, pages\n",
      "3–17. Springer, 1998.\n",
      "Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Joel Leibo, Hubert Soyer, Dharshan Kumaran, and Matthew\n",
      "Botvinick. Meta-reinforcement learning: a bridge between prefrontal and dopaminergic function. In Cosyne\n",
      "Abstracts, 2017.\n",
      "Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014.\n",
      "A Steven Younger, Peter R Conwell, and Neil E Cotter. Fixed-weight on-line learning. IEEE Transactions on\n",
      "Neural Networks, 10(2):272–283, 1999.\n",
      "Barret Zoph and Quoc V Le.\n",
      "Neural architecture search with reinforcement learning.\n",
      "arXiv preprint\n",
      "arXiv:1611.01578, 2016.\n",
      "17\n",
      "\n",
      "------END----------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def get_article(\n",
    "        url: str\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Opens an article using its pdf url and reads its content.\n",
    "    \"\"\"\n",
    "\n",
    "    res = requests.get(url)\n",
    "    if not res.ok:\n",
    "        article = 'Not Found'\n",
    "\n",
    "    else:\n",
    "        bytes_stream = BytesIO(res.content)\n",
    "        with pymupdf.open(stream=bytes_stream) as doc:  \n",
    "            article = chr(12).join([page.get_text() for page in doc])\n",
    "\n",
    "    article = f\"\"\"\n",
    "-------{url}------------\n",
    "{article}\n",
    "------END----------------\n",
    "    \"\"\"\n",
    "\n",
    "    return article\n",
    "\n",
    "print(get_article(\"http://arxiv.org/pdf/1611.05763v3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
